# -*- coding: utf-8 -*-
"""Coursework 1 Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RdFtelGdJfpSKZJNNUJJooIBh0GekexZ
"""



"""<img src="https://www.nlab.org.uk/wp-content/uploads/nlabmain.png" style="width:40%; clear:both; margin-bottom:-20px" align=left>
<br style="clear:both;"/>

# Coursework 1: Customer Segmentation

----------
Dr Georgiana Nica-Avram - University of Nottingham
[mail](mailto:georgiana.nica-avram1@nottingham.ac.uk)

#Initialization
"""

import pandas

# Coursework 1 primer

# This next line is only to be used on Google Colaboratory and will download the CSV file for you

!wget -O asa_cw1_data.zip "https://drive.google.com/uc?export=download&id=1E17e_pf7mu7_cYYh1SaqaOlLPi7jydYR"

!unzip asa_cw1_data.zip

# We can then check that the file is here by listing the content of the current directory
!ls

import pandas

#-- load in the data
filename1 = "asa_cw1_data/baskets_sample.csv"
filename2 = "asa_cw1_data/category_spends_sample.csv"
filename3 = "asa_cw1_data/customers_sample.csv"
filename4 = "asa_cw1_data/lineitems_sample.csv"

baskets_sample = pandas.read_csv(filename1)
category_spends_sample = pandas.read_csv(filename2).set_index('customer_number')
customers_sample = pandas.read_csv(filename3).set_index('customer_number')
lineitems_sample = pandas.read_csv(filename4)

#-- detail the number of datapoints and featuers
print("Number of datapoints %d and number of features %d in the lineitems_sample table" %(lineitems_sample.shape[0], lineitems_sample.shape[1]))
print("Number of datapoints %d and number of features %d in the customers_sample table" %(customers_sample.shape[0], customers_sample.shape[1]))
print("Number of datapoints %d and number of features %d in the baskets_sample table" %(baskets_sample.shape[0], baskets_sample.shape[1]))
print("Number of datapoints %d and number of features %d in the category_spends_sample table" %(category_spends_sample.shape[0], category_spends_sample.shape[1]))

"""# Part 1: Table Exploration

## Data Specifications:
lineitems_sample : 1,461,315 datapoints (products purchased)

customer_sample : 3,000 datapoints (3,000 unique customers)

baskets_sample : 195,547 datapoints (total baskets purchased)

category_spend_sample : 3,000 (amount spent in each category?)

We do not have basket_id uniquely linked to each customer visit. We have the customer number and a 'purchase_time' to use as a unique identifier for each visit

##Tables Visualized

### baskets_sample table

---
"""

# This file details information about each individual visit made by the 3000 customer’s in the sample.
# Please note we don't have a basket_Id uniquely idntifying each visit. Instead, we can use the customer number in conjunction with purchase time as unique identifiers of each visit.

# Replace £ values in the basket_spend column
baskets_sample[baskets_sample.columns[3]]= baskets_sample[baskets_sample.columns[3]].replace('[\£,]', '', regex=True).astype(float)
baskets_sample.sample(3)

"""### category_spends_sample table

Turning the various string columns into floats


*   Remove pound sybol
*   Change type to float
"""

# This file again lists the 3000 customers in the sample, but this time it splits down their spend over the period into 20 item categories

# Replace '£' and ',' in all columns and convert to float
category_spends_sample = category_spends_sample.replace('[\£,]', '', regex=True).astype(float)

# Display a sample
category_spends_sample.sample(3)

# This file again lists the 3000 customers in the sample, but this time it splits down their spend over the period into 20 item categories

print(category_spends_sample.dtypes)
category_spends_sample.head(3)

"""## customers_sample table"""

# A summary file detailing the consumer behaviour of 3000 customers (referenced by an anonymized but consistent “customer_number”)

customers_sample = customers_sample.replace('[\£,]', '', regex=True).astype(float)

customers_sample.head(3)

"""## lineitems_sample table"""

# A final dataset is also available for use that breaks down each basket into its individual product purchase ids, along with category the item belongs to.

# Replace '£' and ',' in the 'spend' column and convert to float
lineitems_sample['spend'] = lineitems_sample['spend'].replace('[\£,]', '', regex=True).astype(float)

lineitems_sample.head(3)

lineitems_sample.head(3)

#Code below is sample of how to change the cast type

baskets_sample['basket_spend'] = baskets_sample['basket_spend'].replace('[\£,]', '', regex=True).astype(float)
baskets_sample.head(3)

"""#Data Cleaning

Filling in 'bakery' data in category_spends_sample with bakery data in lineitems_sample
"""

import pandas as pd

lineitems_sample["spend"] = lineitems_sample["spend"].replace("£", "", regex=True).astype(float)

# Finding the bakery spend per customer
bakery_spend = lineitems_sample[lineitems_sample['category'] == 'BAKERY'].groupby("customer_number")["spend"].sum()

# Assign Bakery spend to category_spends_sample
category_spends_sample["bakery"] = bakery_spend

# Ensure that the missing values are filled with 0 not anything else these
category_spends_sample["bakery"] = category_spends_sample["bakery"].fillna(0)

# Convert any strings in the bakery column to floats
category_spends_sample["bakery"] = category_spends_sample["bakery"].astype(str)  # Convert to string to clean
category_spends_sample["bakery"] = category_spends_sample["bakery"].str.replace("£", "", regex=True)  # Remove any pound
category_spends_sample["bakery"] = category_spends_sample["bakery"].astype(float)  # Convert to float


category_spends_sample.head()

"""Aggregating Tables"""

# Engineer new features from existing tables. Save newly-engineered features in new dataframe.

# Create a new dataframe of purchased quantities across product categries
# the unstack() function is the equivalent of a pivot, so we get a column for every unique product category in the table
lineitems_sample_quantity= lineitems_sample.groupby(['customer_number','category'])['quantity'].sum().unstack('category', fill_value=0).add_prefix('QUANTITY_')
lineitems_sample_quantity.reset_index()

lineitems_sample_quantity.head()

# Inner-join dataframes using Python syntax.
# An inner-join selects all rows from both participating tables as long as there is a matching column between them (index) - in this case 'customer_number' is common in both
# Also note that spend across product categories and quantities across product categories are aggregated at customer level, so both category_spends_sample and lineitems_sample_quantity have 3,000 records

# Join the newly created lineitems_sample_quantity showing purchased quantities across product categories, with the category_spends_sample table (which shows £ spend across product categories)
category_spend_quantity= category_spends_sample.merge(lineitems_sample_quantity, how= 'inner', left_index=True, right_index=True)
# The resulting table now has 3,000 rows (records) and 40 columns (features)

# Further merge the resulting category_spend_quantity table with information on baskets,	total_quantity,	average_quantity,	total_spend,	average_spend for each customer from the customer_sample table
# Again, note that both tables (category_spend_quantity and customer_sample) can be joined on a common column (customer_number)
category_spend_quantity_customers= category_spend_quantity.merge(customers_sample, how= 'inner', left_index=True, right_index=True)

# The resulting tables now has 3,000 rows (records) and 45 columns (features)
print("Number of datapoints: %d and number of features: %d in the category_spend_quantity_customers table" %(category_spend_quantity_customers.shape[0], category_spend_quantity_customers.shape[1]))

# Step 1: Aggregate the baskets_sample table at the customer level
# Summing the numeric columns by customer_number
baskets_sample_sum = baskets_sample.groupby('customer_number')[['basket_quantity', 'basket_spend', 'basket_categories']].sum().add_prefix('SUM_')
print("Number of datapoints %d and number of features %d in the baskets_sample_sum table" % (baskets_sample_sum.shape[0], baskets_sample_sum.shape[1]))
baskets_sample_sum.head(10)

# Step 2: Join the aggregated baskets data with category_spend_quantity_customers on customer_number
customers_baskets_categories_lines = baskets_sample_sum.merge(category_spend_quantity_customers, how='inner', left_index=True, right_index=True)
print("Number of datapoints %d and number of features %d in the customers_baskets_categories_lines table" % (customers_baskets_categories_lines.shape[0], customers_baskets_categories_lines.shape[1]))
customers_baskets_categories_lines.head(10)

# Step 3: Convert object columns to numeric (float) for calculations
# This ensures that any currency or comma formatting is removed before conversion
#numeric_cols = ['average_spend', 'total_spend'] + list(category_spends_sample.columns)  # Include category columns
#for col in numeric_cols:
    #customers_baskets_categories_lines[col] = pandas.to_numeric(customers_baskets_categories_lines[col].str.replace('[\£,]', '', regex=True), errors='coerce')

# Step 4: Recalculate the average quantity and average spend per basket
#customers_baskets_categories_lines['average_quantity'] = customers_baskets_categories_lines['total_quantity'] / customers_baskets_categories_lines['baskets']
#customers_baskets_categories_lines['average_spend'] = customers_baskets_categories_lines['total_spend'] / customers_baskets_categories_lines['baskets']

# Step 5: Final shape and preview of the DataFrame
print("Number of datapoints %d and number of features %d in the customers_baskets_categories_lines table" % (customers_baskets_categories_lines.shape[0], customers_baskets_categories_lines.shape[1]))
customers_baskets_categories_lines.head(10)

# Step 1: Aggregate the baskets_sample table at the customer level
# Summing the numeric columns by customer_number
baskets_sample_sum = baskets_sample.groupby('customer_number')[['basket_quantity', 'basket_spend', 'basket_categories']].sum().add_prefix('SUM_')
print("Number of datapoints %d and number of features %d in the baskets_sample_sum table" % (baskets_sample_sum.shape[0], baskets_sample_sum.shape[1]))
baskets_sample_sum.head(10)

# Step 2: Join the aggregated baskets data with category_spend_quantity_customers on customer_number
customers_baskets_categories_lines = baskets_sample_sum.merge(category_spend_quantity_customers, how='inner', left_index=True, right_index=True)
print("Number of datapoints %d and number of features %d in the customers_baskets_categories_lines table" % (customers_baskets_categories_lines.shape[0], customers_baskets_categories_lines.shape[1]))
customers_baskets_categories_lines.head(10)

# Step 3: Convert object columns to numeric (float) for calculations
# This ensures that any currency or comma formatting is removed before conversion
#numeric_cols = ['average_spend', 'total_spend'] + list(category_spends_sample.columns)  # Include category columns
#for col in numeric_cols:
    #customers_baskets_categories_lines[col] = pandas.to_numeric(customers_baskets_categories_lines[col].str.replace('[\£,]', '', regex=True), errors='coerce')

# Step 4: Recalculate the average quantity and average spend per basket
#customers_baskets_categories_lines['average_quantity'] = customers_baskets_categories_lines['total_quantity'] / customers_baskets_categories_lines['baskets']
#customers_baskets_categories_lines['average_spend'] = customers_baskets_categories_lines['total_spend'] / customers_baskets_categories_lines['baskets']

# Step 5: Final shape and preview of the DataFrame
print("Number of datapoints %d and number of features %d in the customers_baskets_categories_lines table" % (customers_baskets_categories_lines.shape[0], customers_baskets_categories_lines.shape[1]))
customers_baskets_categories_lines.head(10)

"""Checking for Null Values"""

print("Missing Values before treatment:")
print(customers_baskets_categories_lines.isnull().sum())

"""Outlier Detection"""

import numpy as np

def identify_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    print(f"Outliers in {column}: {len(outliers)}")
    return outliers

numeric_columns = customers_baskets_categories_lines.select_dtypes(include=[np.number]).columns

for column in numeric_columns:
    identify_outliers(customers_baskets_categories_lines, column)

import numpy as np

def identify_high_outliers(df, column):
    threshold = df[column].quantile(0.99)
    outliers = df[df[column] > threshold]
    print(f"Outliers above 99th percentile in {column}: {len(outliers)}")
    return outliers

numeric_columns = customers_baskets_categories_lines.select_dtypes(include=[np.number]).columns

for column in numeric_columns:
    identify_high_outliers(customers_baskets_categories_lines, column)

"""**Reasons for capping at 99th percentile:**


Winsorizing prevents outliers from dominating cluster centroids.
More well-separated clusters typically lead to a better silhouette score.
Instead of dropping data, we retain most of your observations but prevent extreme values from skewing your clusters.


"""

import numpy as np

def cap_outliers(df, column, percentile=0.99):
    upper_limit = df[column].quantile(percentile)
    df[column] = np.where(df[column] > upper_limit, upper_limit, df[column])

# Select numeric columns
numeric_columns = customers_baskets_categories_lines.select_dtypes(include=[np.number]).columns

# Apply capping function to each numeric column
for column in numeric_columns:
    cap_outliers(customers_baskets_categories_lines, column)

# The dataset 'customers_baskets_categories_lines' is now modified with outliers capped at the 99th percentile

"""Findings in Outliers:



*   tobacco, drinks, deli, lottery, cashpoint all have high outliers in spending, that means that most of the spending done in these categories is done by certain individuals



What do these outliers tell us? These are people who spend lots of money on certain products (the spend outliers -- high value customers) or customers who habitiually purchase the same thing in quantity on the majority of their trips (quantity outliers -- habitual shoppers)

Different ways to deal with outliers:

Keep them

Get rid of them (cap at max: 3 standard deviations from the mean)

"total sum of spending across all categories, then remove customers that are spending WELL ABOVE THE AVERAGE"

For the purposes of our business analysis, we will keep the outliers. These high value, reoccuring customers are valuable data entrier for our cluster groups. It may be in our best interest to get rid of some of the EXTREME case outliers... we could potentially build a model that creates clusters without these large outliers included and compare performance to the base model.

**Why we need to keep outliers**

Business Relevance:

These "outliers" might represent high-value customers who spend more frequently or in bulk.
Removing them could eliminate valuable segments like premium shoppers or wholesale buyers.
Retail & Grocery Nature:

Large transactions are expected in grocery chains due to bulk purchases, seasonal shopping, or corporate buyers.
Outliers in tobacco, lottery, and drinks could indicate regular large-scale buyers (e.g., convenience store owners).
Cluster Analysis Needs Diversity:

Clustering should reflect real business segments—excluding high-spending customers could bias the analysis toward average shoppers.
If the goal is to identify and target valuable customer groups, then keeping high-spending customers is essential.

**Dealing with the outliers:**


*   Keep them, but cap them at 99th %

Lottery winners: we are deleting lottery winners because we are only concerned with spending, and negative values could influence our clusters in a way that would adversely influence what we are actually trying to find.

We delete 29 customers who won lottery, this should not adversely affect the robustness of our sample
"""

customers_baskets_categories_lines = customers_baskets_categories_lines[customers_baskets_categories_lines['lottery'] >= 0]

customers_baskets_categories_lines.describe()

"""Statistical Summary Findings:



*   Mean SUM_basket_quantity (6 month period) = 584 items [on average people purchased 584 items over the period]
*   Mean SUM_basket_spend (6 month period) = 769 pounds [on average, people spent 769 pounds over the period]
* Mean baskets = 487
* This means that on average, a person spent only 1.31 pounds per item, and 1.58 pounds per basket
* Mean average_quantity = 1.2, so most shoppers are only purchasing 1 item per basket, on average

Point found from graph:


*   Tobacco is relatively low quantity but VERY HIGH VALUE. Marketing of customers who like these products could be crucial
*   Meat & drinks is also relatively low average quantity but high average spend
* Dairy & Grocery: relatively inexpensive consumer staples
* Grocery_health_pets and confectionary are staples (high quantity) and relatively high value, good products
*cashpoint - very low quantity but very high spend

Relatively unimportant categories:
* bakery high quantity but very low spend
* deli
* discount bakery
* practical_items
* seasonal_gifting
"""

#statistical summary of the dataframe "customers_baskets_categories_lines

customers_baskets_categories_lines.describe()

import matplotlib.pyplot as plt

# Select specific columns and calculate averages
category_averages = customers_baskets_categories_lines[['fruit_veg', 'dairy', 'confectionary', 'grocery_food', 'grocery_health_pets', 'bakery', 'newspapers_magazines', 'prepared_meals', 'soft_drinks', 'frozen', 'meat', 'tobacco', 'drinks', 'deli', 'world_foods', 'lottery', 'cashpoint', 'seasonal_gifting', 'discount_bakery', 'practical_items']].replace('[\£,]', '', regex=True).astype(float).mean()

# Sort categories alphabetically
category_averages = category_averages.sort_index()

# Create the bar plot
plt.figure(figsize=(10, 6))
category_averages.plot(kind='bar')
plt.title('Average Spend per Category')
plt.xlabel('Category')
plt.ylabel('Average Spend')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()


# Calculate the average of each quantity category
quantity_averages = customers_baskets_categories_lines.filter(regex='^QUANTITY_.+$').mean()

# Sort categories alphabetically
quantity_averages = quantity_averages.sort_index()

# Create the bar plot for quantity category averages
plt.figure(figsize=(10, 6))
quantity_averages.plot(kind='bar')
plt.title('Average Quantity per Category')
plt.xlabel('Category')
plt.ylabel('Average Quantity')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""Box Plots of Category Spend Data"""

import matplotlib.pyplot as plt
import seaborn as sns

category_spend_data = customers_baskets_categories_lines[['fruit_veg', 'dairy', 'confectionary', 'grocery_food', 'grocery_health_pets',
                                           'newspapers_magazines', 'prepared_meals', 'soft_drinks', 'frozen',
                                           'meat', 'tobacco', 'drinks', 'deli', 'world_foods', 'lottery', 'cashpoint',
                                           'seasonal_gifting', 'discount_bakery', 'practical_items', 'bakery']]

fig, ax= plt.subplots(figsize=(20, 10))
boxplots= sns.boxplot(data=category_spend_data, orient='h', ax=ax)
plt.xlabel('Boxplots of product category variables')

"""Box Plots of Category Quantity Data"""

import matplotlib.pyplot as plt
import seaborn as sns

category_quantity_data = customers_baskets_categories_lines.filter(regex='^QUANTITY_.+$')

fig, ax= plt.subplots(figsize=(20, 10))
boxplots= sns.boxplot(data=category_quantity_data, orient='h', ax=ax)
plt.xlabel('Boxplots of product category variables')

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix


# Select numerical columns
numeric_columns = category_spend_data.select_dtypes(include=['float64', 'int64']).columns

# Calculate correlation matrix
corr = category_spend_data[numeric_columns].corr()

# Create a mask for the upper triangle (True values will hide the cells)
# Exclude the diagonal by setting `k=1` in `np.triu`
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)

# Set up the figure
fig, ax = plt.subplots(figsize=(20, 10))

# Generate diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Generate the heatmap with mask
sns.heatmap(corr, cmap=cmap, mask=mask, vmax=.99, square=True, linewidths=.5, cbar_kws={"shrink": .5}, ax=ax, annot=True, fmt='.2f')
plt.title('Lower Triangle Correlation Heatmap')
plt.xlabel('Market Features Correlations')

# Adjust layout
plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)
plt.show()


def lower_triangle_scatter(df, columns, figsize=(20, 20)):
    num_cols = len(columns)
    fig, axes = plt.subplots(num_cols, num_cols, figsize=figsize)
    plt.suptitle('Lower Triangle Scatter Matrix of Market Features', fontsize=16)

    for i in range(num_cols):
        for j in range(num_cols):
            if i > j:
                axes[i, j].scatter(df[columns[j]], df[columns[i]], alpha=0.7, marker='o', s=20)
                axes[i, j].set_xlabel(columns[j])
                axes[i, j].set_ylabel(columns[i])
            elif i == j:
                df[columns[i]].plot(kind='kde', ax=axes[i, j], color='orange')
                axes[i, j].set_ylabel('Density')
            else:
                axes[i, j].axis('off')

    # Adjust layout
    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.4, hspace=0.4)
    plt.show()

# Split the columns into chunks and create separate figures
chunk_size = 5
for i in range(0, len(numeric_columns), chunk_size):
    subset_columns = numeric_columns[i:i + chunk_size]
    lower_triangle_scatter(category_spend_data, subset_columns, figsize=(20, 20))

"""Lets combine spend categories, as there is too much noise in this data and too many categories will hurt the accuracy of our clusters

Combining Spend categories
"""

#Creating new spend categories in customers_baskets_categories_lines

#create new spend category 'fresh_foods' that combines (fruit_veg, dairy, grocery_food, meat, discount_bakery, bakery)
#create new spend category 'processed_foods' that combines (confectionary, prepared_meals, soft_drinks, frozen, deli, drinks)
#create new spend category 'non_food_items' that combines (newspaper_magazines, tobacco, lottery, cash_point, seasonal_gifting, practical_items)

# Create 'fresh_foods' category
customers_baskets_categories_lines['fresh_foods'] = (
    customers_baskets_categories_lines['fruit_veg'] +
    customers_baskets_categories_lines['dairy'] +
    customers_baskets_categories_lines['grocery_food'] +
    customers_baskets_categories_lines['meat'] +
    customers_baskets_categories_lines['discount_bakery'] +
    customers_baskets_categories_lines['bakery']
)

# Create 'processed_foods' category
customers_baskets_categories_lines['processed_foods'] = (
    customers_baskets_categories_lines['confectionary'] +
    customers_baskets_categories_lines['prepared_meals'] +
    customers_baskets_categories_lines['soft_drinks'] +
    customers_baskets_categories_lines['frozen'] +
    customers_baskets_categories_lines['deli'] +
    customers_baskets_categories_lines['drinks']+
    customers_baskets_categories_lines['world_foods']
)

# Create 'non_food_items' category
customers_baskets_categories_lines['non_food_items'] = (
    customers_baskets_categories_lines['newspapers_magazines'] +
    customers_baskets_categories_lines['tobacco'] +
    customers_baskets_categories_lines['lottery'] +
    customers_baskets_categories_lines['cashpoint'] +
    customers_baskets_categories_lines['seasonal_gifting'] +
    customers_baskets_categories_lines['practical_items'] +
    customers_baskets_categories_lines['grocery_health_pets']
)
# Remove original category columns after aggregation
columns_to_drop = [
    'fruit_veg', 'dairy', 'grocery_food', 'meat', 'discount_bakery', 'bakery',  # Fresh Foods
    'confectionary', 'prepared_meals', 'soft_drinks', 'frozen', 'deli', 'drinks', 'world_foods',  # Processed Foods
    'newspapers_magazines', 'tobacco', 'lottery', 'cashpoint', 'seasonal_gifting', 'practical_items', 'grocery_health_pets',  # Non-Food Items
]

customers_baskets_categories_lines.drop(columns=columns_to_drop, inplace=True)

# Display the DataFrame with the new categories
print(customers_baskets_categories_lines.head())

customers_baskets_categories_lines.describe()

"""Dropping QUANTITY data because quantity and spend are similar, and the amount spent seems to be of more relevance for customer inforamtion that Quantity. Both achieve the same thing but spend seems more relevant."""



#dropping all instances of QUANTITY in the customers_baskets_categories_lines

customers_baskets_categories_lines = customers_baskets_categories_lines.drop(columns=customers_baskets_categories_lines.filter(regex='QUANTITY').columns)

customers_baskets_categories_lines.head()

customers_baskets_categories_lines.head()

"""#Feature Engineering"""

!pip install pyspark

# Generate'average_visits_monthly' and merge with the dataframe 'customers_baskets_categories_lines' excluding any missing 'customer_number'
from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Initialize SparkSession
spark = SparkSession.builder.appName("AverageVisits").getOrCreate()

# Convert pandas DataFrame to Spark DataFrame
baskets_sample_spark = spark.createDataFrame(baskets_sample)
baskets_sample_spark.createOrReplaceTempView("baskets_sample")

# Step 1: Group by customer_number and month to calculate visits per month
monthly_visits = (spark.table("baskets_sample")
                  .withColumn("month", F.date_trunc("month", F.col("purchase_time")))
                  .groupBy("customer_number", "month")
                  .agg(F.countDistinct("purchase_time").alias("visits_per_month")))

# Step 2: Calculate the average number of visits per month for each customer
average_visits = (monthly_visits
                  .groupBy("customer_number")
                  .agg(F.avg("visits_per_month").alias("avg_visits_monthly")))

# Step 3: Remove any missing customer_number values
average_visits = average_visits.dropna(subset=["customer_number"])

# Convert to Pandas for merging
average_visits_pd = average_visits.toPandas()

# Step 4: Merge onto customers_baskets_categories_lines DataFrame
customers_baskets_categories_lines = customers_baskets_categories_lines.merge(
    average_visits_pd, on="customer_number", how="left"
)

#Check results
print(customers_baskets_categories_lines.head())

import pandas as pd

# Renaming DataFrames to match the user's dataset names
customers_df = customers_sample
category_spends_df = category_spends_sample
baskets_df = baskets_sample
lineitem_df = lineitems_sample

#  Convert relevant columns to float before Feature Engineering

# Convert 'total_spend', 'total_quantity', and 'baskets' to float
customers_df['total_spend'] = pd.to_numeric(customers_df['total_spend'], errors='coerce')
customers_df['total_quantity'] = pd.to_numeric(customers_df['total_quantity'], errors='coerce')
customers_df['baskets'] = pd.to_numeric(customers_df['baskets'], errors='coerce')


#  Feature Engineering

# 1. Average Spend per Item (Total Spend / Total Quantity)
customers_df['avg_spend_per_item'] = customers_df['total_spend'] / customers_df['total_quantity']
customers_df['avg_spend_per_item'].fillna(0, inplace=True)  # Handle division by zero

# 2. Average Basket Spend (Total Spend / Number of Baskets)
customers_df['avg_basket_spend'] = customers_df['total_spend'] / customers_df['baskets']
customers_df['avg_basket_spend'].fillna(0, inplace=True)

# 3. Average Item Count per Basket (Total Quantity / Number of Baskets)
customers_df['avg_item_count'] = customers_df['total_quantity'] / customers_df['baskets']
customers_df['avg_item_count'].fillna(0, inplace=True)

# 4. Spend Concentration (Top 3 Categories / Total Spend)
top_categories = category_spends_df.apply(lambda x: x.nlargest(3).sum(), axis=1) # Removed drop(columns=['customer_number'])
customers_df['spend_concentration'] = top_categories / customers_df['total_spend']
customers_df['spend_concentration'].fillna(0, inplace=True)

# 5. Recency of Last Purchase (Days Since Last Purchase)
baskets_df['purchase_time'] = pd.to_datetime(baskets_df['purchase_time'])  # Ensure datetime format
latest_date = baskets_df['purchase_time'].max()
customer_recency = baskets_df.groupby('customer_number')['purchase_time'].max().apply(lambda x: (latest_date - x).days)

# Reset the index of customers_df to make 'customer_number' a column
customers_df = customers_df.reset_index()

# Apply the map function
customers_df['recency_last_purchase'] = customers_df['customer_number'].map(customer_recency)
customers_df['recency_last_purchase'].fillna(customers_df['recency_last_purchase'].median(), inplace=True)

# 6. Weekend vs. Weekday Shopper (Proportion of Purchases on Weekends)
baskets_df['weekend'] = baskets_df['purchase_time'].dt.weekday >= 5  # 0-4 = Weekdays, 5-6 = Weekends
weekend_counts = baskets_df.groupby('customer_number')['weekend'].mean()  # Percentage of visits on weekends
customers_df['weekend_shopper'] = customers_df['customer_number'].map(weekend_counts)
customers_df['weekend_shopper'].fillna(0, inplace=True)

# 7. Time of Day Preference (Morning vs. Evening Purchases)
baskets_df['hour'] = baskets_df['purchase_time'].dt.hour
baskets_df['morning'] = baskets_df['hour'].between(6, 12)  # Define morning hours
morning_counts = baskets_df.groupby('customer_number')['morning'].mean()
customers_df['morning_shopper'] = customers_df['customer_number'].map(morning_counts)
customers_df['morning_shopper'].fillna(0, inplace=True)

# 8. Category Specialization Score (Purchase Diversity)
category_diversity = baskets_df.groupby('customer_number')['basket_categories'].mean()
customers_df['category_specialization'] = customers_df['customer_number'].map(category_diversity)
customers_df['category_specialization'].fillna(customers_df['category_specialization'].median(), inplace=True)

# 9. Churn Risk (Long Gaps Between Visits)
visit_gaps = baskets_df.sort_values(['customer_number', 'purchase_time']).groupby('customer_number')['purchase_time'].diff().dt.days
avg_gap = visit_gaps.groupby(baskets_df['customer_number']).mean()
customers_df['churn_risk'] = customers_df['customer_number'].map(avg_gap)
customers_df['churn_risk'].fillna(customers_df['churn_risk'].median(), inplace=True)

# Display engineered features
print("Engineered Customer Features:")
print(customers_df)  # Use print(customers_df) instead of tools.display_dataframe_to_user

# Display newly created variables
new_features = ['avg_spend_per_item', 'avg_basket_spend', 'avg_item_count', 'spend_concentration',
                'recency_last_purchase', 'weekend_shopper', 'morning_shopper', 'category_specialization', 'churn_risk']
print("\nNewly Created Features:")
print(customers_df[new_features])

"""Take these newly generated features and add them to our table"""

# Ensure customer_number exists in both DataFrames
customers_df = customers_df.dropna(subset=['customer_number'])

# Select only customer_number + newly generated features
new_features = [
    'customer_number', 'avg_spend_per_item', 'avg_basket_spend', 'avg_item_count',
    'spend_concentration', 'recency_last_purchase', 'weekend_shopper',
    'morning_shopper', 'category_specialization', 'churn_risk'
]
customers_features_df = customers_df[new_features]

# Merge onto customers_baskets_categories_lines, performing a left join
customers_baskets_categories_lines = customers_baskets_categories_lines.merge(
    customers_features_df, on="customer_number", how="left"
)

# Display results
print(customers_baskets_categories_lines.head())  # Check if merge is successful

# Let's list all the columns in the big table we just generated:
list(customers_baskets_categories_lines.columns.values)

"""#Data Visualization

Data Correllation and Scatter Plots:
"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix


# Select numerical columns
numeric_columns = customers_baskets_categories_lines.select_dtypes(include=['float64', 'int64']).columns

# Calculate correlation matrix
corr = customers_baskets_categories_lines[numeric_columns].corr()

# Create a mask for the upper triangle (True values will hide the cells)
# Exclude the diagonal by setting `k=1` in `np.triu`
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)

# Set up the figure
fig, ax = plt.subplots(figsize=(20, 10))

# Generate diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Generate the heatmap with mask
sns.heatmap(corr, cmap=cmap, mask=mask, vmax=.99, square=True, linewidths=.5, cbar_kws={"shrink": .5}, ax=ax, annot=True, fmt='.2f')
plt.title('Lower Triangle Correlation Heatmap')
plt.xlabel('Market Features Correlations')

# Adjust layout
plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)
plt.show()


def lower_triangle_scatter(df, columns, figsize=(20, 20)):
    num_cols = len(columns)
    fig, axes = plt.subplots(num_cols, num_cols, figsize=figsize)
    plt.suptitle('Lower Triangle Scatter Matrix of Market Features', fontsize=16)

    for i in range(num_cols):
        for j in range(num_cols):
            if i > j:
                axes[i, j].scatter(df[columns[j]], df[columns[i]], alpha=0.7, marker='o', s=20)
                axes[i, j].set_xlabel(columns[j])
                axes[i, j].set_ylabel(columns[i])
            elif i == j:
                df[columns[i]].plot(kind='kde', ax=axes[i, j], color='orange')
                axes[i, j].set_ylabel('Density')
            else:
                axes[i, j].axis('off')

    # Adjust layout
    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.4, hspace=0.4)
    plt.show()

# Split the columns into chunks and create separate figures
chunk_size = 5
for i in range(0, len(numeric_columns), chunk_size):
    subset_columns = numeric_columns[i:i + chunk_size]
    lower_triangle_scatter(customers_baskets_categories_lines, subset_columns, figsize=(20, 20))

"""Deleting highly correllated features"""

#delete features (basket_quantity, SUM_basket_categories, baskets, total_quantity, average_quantity, average_basket_spend) from dataframe customers_baskets_categories_lines
# List of columns to drop
# Include 'SUM_basket_spend' in the list of columns to drop
columns_to_drop = ['SUM_basket_quantity','avg_spend_per_item', 'SUM_basket_spend', 'SUM_basket_categories', 'baskets', 'total_quantity', 'average_quantity', 'average_spend']

# Drop the columns
customers_baskets_categories_lines = customers_baskets_categories_lines.drop(columns=columns_to_drop, errors='ignore') # Use errors='ignore' to avoid KeyError if a column is not found

# Display the updated DataFrame (optional)
print(customers_baskets_categories_lines.head())

"""Final Heatmap"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix

# Select numerical columns excluding 'customer_number'
numeric_columns = customers_baskets_categories_lines.select_dtypes(include=['float64', 'int64']).columns
numeric_columns = numeric_columns.drop('customer_number', errors='ignore')  # Drop only if it exists

# Calculate correlation matrix
corr = customers_baskets_categories_lines[numeric_columns].corr()

# Create a mask for the upper triangle (True values will hide the cells)
# Exclude the diagonal by setting `k=1` in `np.triu`
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)

# Set up the figure
fig, ax = plt.subplots(figsize=(20, 10))

# Generate diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Generate the heatmap with mask
sns.heatmap(corr, cmap=cmap, mask=mask, vmax=.99, square=True, linewidths=.5, cbar_kws={"shrink": .5}, ax=ax, annot=True, fmt='.2f')
plt.title('Lower Triangle Correlation Heatmap')
plt.xlabel('Market Features Correlations')

# Adjust layout
plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)
plt.show()


def lower_triangle_scatter(df, columns, figsize=(20, 20)):
    num_cols = len(columns)
    fig, axes = plt.subplots(num_cols, num_cols, figsize=figsize)
    plt.suptitle('Lower Triangle Scatter Matrix of Market Features', fontsize=16)

    for i in range(num_cols):
        for j in range(num_cols):
            if i > j:
                axes[i, j].scatter(df[columns[j]], df[columns[i]], alpha=0.7, marker='o', s=20)
                axes[i, j].set_xlabel(columns[j])
                axes[i, j].set_ylabel(columns[i])
            elif i == j:
                df[columns[i]].plot(kind='kde', ax=axes[i, j], color='orange')
                axes[i, j].set_ylabel('Density')
            else:
                axes[i, j].axis('off')

    # Adjust layout
    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.4, hspace=0.4)
    plt.show()

# Split the columns into chunks and create separate figures
chunk_size = 5
for i in range(0, len(numeric_columns), chunk_size):
    subset_columns = numeric_columns[i:i + chunk_size]
    lower_triangle_scatter(customers_baskets_categories_lines, subset_columns, figsize=(20, 20))

"""##Exploratory analysis with final set of features

Graphs of main features
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


df = customers_baskets_categories_lines

sns.set(style="whitegrid")

# 1. Distribution of Total Spend Across Customers
plt.figure(figsize=(10, 6))
sns.histplot(df['total_spend'], bins=50, kde=True)
plt.title("Distribution of Total Spend Across Customers")
plt.xlabel("Total Spend (£)")
plt.ylabel("Number of Customers")
plt.show()

# 2. Average Basket Spend Distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['avg_basket_spend'], bins=50, kde=True)
plt.title("Distribution of Average Basket Spend")
plt.xlabel("Average Basket Spend (£)")
plt.ylabel("Number of Customers")
plt.show()

# 3. Category-Level Spending
plt.figure(figsize=(10, 6))
df[['fresh_foods', 'processed_foods', 'non_food_items']].mean().plot(kind='bar')
plt.title("Average Spending per Category")
plt.xlabel("Category")
plt.ylabel("Average Spend (£)")
plt.xticks(rotation=45)
plt.show()

# 4. Recency of Last Purchase Distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['recency_last_purchase'], bins=50, kde=True)
plt.title("Recency of Last Purchase Distribution")
plt.xlabel("Days Since Last Purchase")
plt.ylabel("Number of Customers")
plt.show()

# 5. Customer Visit Frequency (Average Visits per Month)
plt.figure(figsize=(10, 6))
sns.histplot(df['avg_visits_monthly'], bins=50, kde=True)
plt.title("Customer Visit Frequency (Average Visits per Month)")
plt.xlabel("Average Visits per Month")
plt.ylabel("Number of Customers")
plt.show()

# 6. Churn Risk vs. Total Spend
plt.figure(figsize=(10, 6))
sns.scatterplot(x=df['churn_risk'], y=df['total_spend'], alpha=0.5)
plt.title("Churn Risk vs. Total Spend")
plt.xlabel("Churn Risk (Avg. Days Between Visits)")
plt.ylabel("Total Spend (£)")
plt.show()

# 7. Weekend vs. Weekday Shoppers
plt.figure(figsize=(10, 6))
sns.histplot(df['weekend_shopper'], bins=50, kde=True)
plt.title("Proportion of Weekend Shoppers")
plt.xlabel("Percentage of Purchases on Weekends")
plt.ylabel("Number of Customers")
plt.show()

"""# PCA & NMF

Logging Data and Checking for Normal Distribution
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Replace pound symbols and convert to float
customers_baskets_categories_lines = customers_baskets_categories_lines.replace('[\£,]', '', regex=True).astype(float)

# Log transform the data
logged_data = np.log1p(customers_baskets_categories_lines)

# Select numerical columns, excluding 'customer_number' from correlation analysis
excluded_columns = ['customer_number']
numeric_columns = [col for col in logged_data.select_dtypes(include=['float64', 'int64']).columns if col not in excluded_columns]

# Calculate correlation matrix
corr = logged_data[numeric_columns].corr()

# Create a mask for the upper triangle (True values will hide the cells)
mask = np.triu(np.ones_like(corr, dtype=bool), k=1)

# Set up the figure
fig, ax = plt.subplots(figsize=(20, 10))

# Generate diverging colormap
cmap = sns.diverging_palette(220, 10, as_cmap=True)

# Generate the heatmap with mask
sns.heatmap(corr, cmap=cmap, mask=mask, vmax=.99, square=True, linewidths=.5, cbar_kws={"shrink": .5}, ax=ax, annot=True, fmt='.2f')
plt.title('Lower Triangle Correlation Heatmap')
plt.xlabel('Market Features Correlations')

# Adjust layout
plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)
plt.show()


def lower_triangle_scatter(df, columns, figsize=(20, 20)):
    num_cols = len(columns)
    fig, axes = plt.subplots(num_cols, num_cols, figsize=figsize) # Create subplots
    plt.suptitle('Lower Triangle Scatter Matrix of Market Features', fontsize=16)

    for i in range(num_cols):
        for j in range(num_cols):
            ax = axes[i, j] if num_cols > 1 else axes  # Handle single-column case

            if i > j:
                ax.scatter(df[columns[j]], df[columns[i]], alpha=0.7, marker='o', s=20)
                ax.set_xlabel(columns[j])
                ax.set_ylabel(columns[i])
            elif i == j:
                df[columns[i]].plot(kind='kde', ax=ax, color='orange')
                ax.set_ylabel('Density')
            else:
                ax.axis('off')

    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.4, hspace=0.4)
    plt.show()

# Split the columns into chunks and create separate figures
chunk_size = 5
for i in range(0, len(numeric_columns), chunk_size):
    subset_columns = numeric_columns[i:i + chunk_size]
    lower_triangle_scatter(logged_data, subset_columns, figsize=(20, 20))

"""Standardizing Data

Use MinMaxScaler to Normalize the data. PCA is based on variance and uses Euclidean distance. If your features are on different scales (even after logging), the larger-scaled features will dominate the smaller ones. Normalization (usually StandardScaler or MinMaxScaler) ensures each feature contributes equally.

Variance Expalined Method for choosing number of components for PCA



*   Choose the smallest number of components that explains around 85-95% of the total variance.
"""

# MinMaxScaler on logged_data
from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Fit and transform the data
scaled_data = scaler.fit_transform(logged_data)

# Convert scaled_data back to a Pandas DataFrame with original columns
scaled_df = pd.DataFrame(scaled_data, columns=logged_data.columns, index=logged_data.index)

# Exclude the 'customer_number' column from the dataset
pca_data = scaled_df.drop(columns=['customer_number'])

# Fit PCA on your scaled, logged data
from sklearn.decomposition import PCA
import numpy as np
import matplotlib.pyplot as plt

pca = PCA()
pca.fit(pca_data)

# Calculate cumulative explained variance
explained_var = np.cumsum(pca.explained_variance_ratio_)  # Fixed typo: 'explained' not 'explained'

# Automatically find number of components for 90% variance
n_components = np.argmax(explained_var >= 0.90)
print(f"Number of components to explain 90% variance: {n_components}")

# Transform data using optimal components
X_pca = pca.transform(pca_data)[:, :n_components]

# Optional Scree Plot
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.axhline(y=0.90, color='r', linestyle='--', label='90% Threshold')
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.title("Scree Plot")
plt.grid()
plt.legend()
plt.show()

# Apply PCA with 7 components for further analysis
pca = PCA(n_components=7)
pca.fit(pca_data)  # Use the scaled and logged dataset without 'customer_number'

# Create DataFrame with PCA components (loadings)
pca_results = pd.DataFrame(
    data=pca.components_,
    columns=pca_data.columns,  # Use the columns from the modified dataset
    index=[f'Dimension {i+1}' for i in range(pca.n_components_)] # Fix: Use pca.n_components_ to dynamically generate the correct number of dimensions for the index
)

# Display PCA components (loadings)
print("PCA Component Loadings:")
print(pca_results)

# Set up a diverging color palette
import seaborn as sns  # For better color palettes
diverging_palette = sns.color_palette("Spectral", n_colors=len(pca_data.columns))  # Spectral is a good diverging palette

# Plot PCA loadings as bar charts in 2 separate graphs
fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(20, 20))

# Plot the first 6 components
pca_results.iloc[:3].plot(kind='bar', ax=axes[0], legend=True, color=diverging_palette)
axes[0].set_title("PCA Component Loadings (Dimensions 1-3)")
axes[0].set_xlabel("Features")
axes[0].set_ylabel("Loading Value")
axes[0].grid(True)

# Plot the next 6 components
pca_results.iloc[4:6].plot(kind='bar', ax=axes[1], legend=True, color=diverging_palette)
axes[1].set_title("PCA Component Loadings (Dimensions 4-6)")
axes[1].set_xlabel("Features")
axes[1].set_ylabel("Loading Value")
axes[1].grid(True)

plt.tight_layout()
plt.show()

# Explained Variance Ratio (How much variance each component explains)
print("\nExplained Variance Ratio for each component:")
print(pca.explained_variance_ratio_)

# Cumulative Explained Variance (How much total variance is explained by the first N components)
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)
print("\nCumulative Explained Variance:")
print(cumulative_variance)

"""##NMF





* LDA maximizes class separability, making it ideal for clustering if you have labeled data.

* It can produce more interpretable projections when class labels are available.


"""

from sklearn.decomposition import NMF
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np

# Drop 'customer_number' before normalization
data_for_nmf = customers_baskets_categories_lines.drop(columns=['customer_number'], errors='ignore')

# Scale between 0 and 1 (MinMaxScaler ensures all values are non-negative)
scaler = MinMaxScaler()
normalized_data = scaler.fit_transform(data_for_nmf)

# Convert back to DataFrame
normalized_data = pd.DataFrame(normalized_data, columns=data_for_nmf.columns)

# Perform NMF
n_components = 5
nmf = NMF(n_components=n_components, random_state=42)
W = nmf.fit_transform(normalized_data)  # Feature weights
H = nmf.components_  # Component scores

# Create DataFrame for NMF components
nmf_results = pd.DataFrame(H, columns=normalized_data.columns, index=[f'NMF Component {i+1}' for i in range(n_components)])

# Feature Selection via Component Loadings
top_features_per_component = 3
selected_features = set()

for i, component in enumerate(H):
    top_features = np.argsort(component)[-top_features_per_component:]  # Select highest loadings
    selected_features.update(normalized_data.columns[top_features])
    print(f"Top {top_features_per_component} Features for NMF Component {i+1}: {normalized_data.columns[top_features].values}")

selected_features = list(selected_features)
print("\nFinal Selected Features:", selected_features)

# Create dataset with only selected features
X_selected = normalized_data[selected_features]

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Usinga distinct color pallette to differentiate clusters
palette = sns.color_palette("Set1", n_colors=5)

# Create a scatter plot with distinct colors for each cluster
plt.figure(figsize=(10, 6))

for cluster_num in range(5):
    plt.scatter(
        W_pca[final_cluster_labels == cluster_num, 0],
        W_pca[final_cluster_labels == cluster_num, 1],
        label=f"Cluster {cluster_num+1}",  # Numbering clusters from 1 to 5
        alpha=0.7,
        color=palette[cluster_num]
    )

# Compute cluster centroids and label them
centroids = np.array([W_pca[final_cluster_labels == i].mean(axis=0) for i in range(5)])
for i, centroid in enumerate(centroids):
    plt.text(centroid[0], centroid[1], str(i+1), fontsize=15, fontweight='bold',
             ha='center', va='center', color='black', bbox=dict(facecolor='white', alpha=0.6))

# Title and labels
plt.title("K-Means Clustering on Full NMF Representation (k=5)")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")


plt.show()

"""Why This Clustering Looks Better?? Include this info in report


Using the full NMF representation (W) preserves structure and improves separation.


NMF extracts part-based patterns, making clustering more meaningful.


MinMax Scaling ensures feature consistency, avoiding distortions from log transformation.


Applying PCA on W for visualization maintains separability, making clusters look tighter.


K-Means now has better-defined data, improving its performance.
"""

# Merge clusters back to original dataset
if 'customer_number' in customers_baskets_categories_lines.columns:
    clustered_data["customer_number"] = customers_baskets_categories_lines["customer_number"].values
    merged_data = clustered_data.merge(customers_baskets_categories_lines, on="customer_number", how="left")

    # Drop `customer_number` after merging
    merged_data = merged_data.drop(columns=['customer_number'], errors='ignore')

    # Compute cluster-wise feature averages
    cluster_summary = merged_data.groupby("Cluster").mean()

    # Compute relative feature importance per cluster
    relative_importance = cluster_summary.div(cluster_summary.sum(axis=1), axis=0)

    # Save the results
    cluster_summary.to_csv("cluster_summary.csv")
    relative_importance.to_csv("relative_importance.csv")

    print("Cluster Summary (Feature Averages) - `customer_number` Removed")
    print(cluster_summary.head())

    print("\nRelative Feature Importance per Cluster - `customer_number` Removed")
    print(relative_importance.head())

else:
    print("Error: `customer_number` column not found. Please re-upload the dataset.")

"""##Silhouette Scores for Comparison of PCA and NMF

Silhouette Score:

A score close to 1 indicates well-separated clusters.

A score close to 0 indicates overlapping clusters.

A negative score indicates poor clustering.

Silhouette Analysis
"""

from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from sklearn.cluster import KMeans
import numpy as np

# Use NMF-transformed data (W) instead of PCA
X = W

# Range of cluster numbers to test
range_n_clusters = [2, 3, 4, 5, 6, 7, 8]

for n_clusters in range_n_clusters:
    fig, ax = plt.subplots(1, 1, figsize=(10, 6))

    # Perform K-Means Clustering
    clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    cluster_labels = clusterer.fit_predict(X)

    # Calculate the average silhouette score
    silhouette_avg = silhouette_score(X, cluster_labels)
    print(f"For n_clusters = {n_clusters}, the average silhouette score is: {silhouette_avg:.4f}")

    # Calculate silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    # Plot silhouette analysis
    y_lower = 10
    for i in range(n_clusters):
        ith_cluster_silhouette_values = sample_silhouette_values[np.where(cluster_labels == i)]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = len(ith_cluster_silhouette_values)
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)

        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
        y_lower = y_upper + 10

    ax.axvline(x=silhouette_avg, color="red", linestyle="--", label="Average Silhouette Score")
    ax.set_xlabel("Silhouette Coefficient Values")
    ax.set_ylabel("Cluster Label")
    ax.set_title(f"Silhouette Plot for {n_clusters} Clusters")
    ax.legend(loc="upper right")
    ax.set_yticks([])
    plt.show()

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Number of clusters (adjust based on your data)
n_clusters = 5

# Perform K-Means clustering on NMF-transformed data
kmeans_nmf = KMeans(n_clusters=n_clusters, random_state=42)
clusters_nmf = kmeans_nmf.fit_predict(W)  # W is the NMF-transformed data

# Calculate silhouette score for NMF
silhouette_nmf = silhouette_score(W, clusters_nmf)
print(f"Silhouette Score for NMF + K-Means: {silhouette_nmf:.4f}")

# Perform K-Means clustering on PCA-transformed data
kmeans_pca = KMeans(n_clusters=n_clusters, random_state=42)
clusters_pca = kmeans_pca.fit_predict(X_pca)  # X_pca is the PCA-transformed data

# Calculate silhouette score for PCA
silhouette_pca = silhouette_score(X_pca, clusters_pca)
print(f"Silhouette Score for PCA + K-Means: {silhouette_pca:.4f}")

# Compare the scores
if silhouette_nmf > silhouette_pca:
    print("NMF performs better for clustering.")
elif silhouette_pca > silhouette_nmf:
    print("PCA performs better for clustering.")
else:
    print("NMF and PCA perform equally well for clustering.")

"""Attempting to equally weight all of the variables to see if we get improved clusters or silhouette score."""

# from sklearn.preprocessing import StandardScaler
# from sklearn.cluster import KMeans
# from sklearn.metrics import silhouette_score
# import pandas as pd
# import matplotlib.pyplot as plt
# import seaborn as sns

# # Load the dataset (assuming `customers_baskets_categories_lines` is provided)
# # Dropping 'customer_number' if present
# data_for_clustering = customers_baskets_categories_lines.drop(columns=['customer_number'], errors='ignore')

# # Standardize Data (Equal Weighting)
# scaler = StandardScaler()
# standardized_data = scaler.fit_transform(data_for_clustering)
# standardized_data = pd.DataFrame(standardized_data, columns=data_for_clustering.columns)

# # Test different values of k using silhouette score
# range_n_clusters = [2, 3, 4, 5, 6, 7, 8]
# silhouette_scores = {}

# for n_clusters in range_n_clusters:
#     kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
#     cluster_labels = kmeans.fit_predict(standardized_data)
#     silhouette_avg = silhouette_score(standardized_data, cluster_labels)
#     silhouette_scores[n_clusters] = silhouette_avg
#     print(f"For n_clusters = {n_clusters}, the average silhouette score is: {silhouette_avg:.4f}")

# # Determine the best k based on highest silhouette score
# best_k = max(silhouette_scores, key=silhouette_scores.get)
# print(f"Best number of clusters determined by silhouette score: {best_k}")

# # Final Clustering with best k
# final_kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
# final_cluster_labels = final_kmeans.fit_predict(standardized_data)

# # Create Clustered DataFrame
# clustered_data = standardized_data.copy()
# clustered_data["Cluster"] = final_cluster_labels

# # Visualize Clusters Using PCA
# from sklearn.decomposition import PCA

# pca = PCA(n_components=2)
# pca_data = pca.fit_transform(standardized_data)

# plt.figure(figsize=(10, 6))
# sns.scatterplot(x=pca_data[:, 0], y=pca_data[:, 1], hue=final_cluster_labels, palette="viridis", alpha=0.7)
# plt.title(f"K-Means Clustering with Equal Weighting (k={best_k})")
# plt.xlabel("PCA Component 1")
# plt.ylabel("PCA Component 2")
# plt.legend(title="Cluster")
# plt.show()

"""We do not get better clusters or scores.... don't use this!

#Saving as a CSV
"""

import pandas as pd
import datetime

# Create a DataFrame to store customer assignments
customer_assignments = pd.DataFrame({
    "customer_number": customers_baskets_categories_lines["customer_number"],  # Ensure customer_number exists
    "Cluster": final_cluster_labels + 1  # Adjust to match 1-5 numbering instead of 0-4
})

# Define the cluster names based on segmentation
cluster_names = {
    1: "High Value Shoppers",
    2: "Quick Morning Stop Shoppers",
    3: "Morning Mission Bulk Buyers",
    4: "One Item, Random Stop Shoppers",
    5: "The Moderate Shopper"
}

# Map cluster numbers to descriptive names
customer_assignments["Segment"] = customer_assignments["Cluster"].map(cluster_names)

# Generate a timestamp for the filename
timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
csv_filename = f"customer_segments_{timestamp}.csv"

# Save to CSV with the timestamped filename
customer_assignments.to_csv(csv_filename, index=False)

print(f"CSV file '{csv_filename}' has been created successfully.")