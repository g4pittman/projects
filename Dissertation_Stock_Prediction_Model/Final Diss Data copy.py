# -*- coding: utf-8 -*-
"""2nd Try Diss.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S3_dxTuBdHQt7hqaJyHbrKEAjRdUh7H2

#Loading Data
"""

import pandas as pd

company_metadata_df = pd.read_csv('company_metadata.csv')
daily_master_df = pd.read_csv('daily_master_final.csv')
quarterly_raw_df = pd.read_csv('quarterly_raw.csv')
yield_df = pd.read_csv('T10Y2Y.csv')

#display the first few rows from each dataframe
display(company_metadata_df.head(10))
display(daily_master_df.head(10))
display(quarterly_raw_df.head(10))
display(yield_df.head(10))

#Display basic statistics about each dataframe
display(company_metadata_df.describe())
display(daily_master_df.describe())
display(quarterly_raw_df.describe())
display(yield_df.describe())

"""Changes we need to make:



*   company_metadata_df
  * RD_CREDIT_RATING_GLOBAL -- categorical variable
  * SP_IPO_Date -- lots of NAAN's, also the format is currently: mm/dd/yy "time" and should just be standard date format with no time
  * SP_IPO_Price -- lots of NaaN's
  * SP_IPO_Offering_Price -- lotso f NaaN's
  * SP_COMMON_DIV_DECLARED -- there are Naan's that I am venturing to guess should just be zero's (no dividend means 0, not not available)
*   quarterly_raw_df
  * we have the column "metric" that displays either "book_value_equity" or "net_income", but in reality we should just add these two features as columns since they're present for every stock

##Company_metadata_df changes listed above
"""

#Show the different outputs and total number of each output for variable: RD_CREDIT_RATING_GLOBAL

for i in company_metadata_df['RD_CREDIT_RATING_GLOBAL'].unique():
  print(i, len(company_metadata_df[company_metadata_df['RD_CREDIT_RATING_GLOBAL'] == i]))

#Changing RD_CREDIT_RATING_GLOBAL to categorical ranked variable
#we now have two new columns that we will use in our model: "credit_rating_rank" and "was_not_rated"

import numpy as np
import pandas as pd

df = company_metadata_df

# Step 1: Define ordinal ranking map
credit_rating_map = {
    "AAA": 20,
    "AA+": 19,
    "AA": 18,
    "AA-": 17,
    "A+": 16,
    "A": 15,
    "A-": 14,
    "BBB+": 13,
    "BBB": 12,
    "BBB-": 11,
    "BB+": 10,
    "BB": 9,
    "BB-": 8,
    "B+": 7,
    "B": 6,
    "B-": 5,
    "CCC+": 4,
    "CCC": 3,
    "CCC-": 2,
    "CC": 1,
    "C": 1,
    "D": 0,
    "NR": np.nan  # handled separately below
}

# Step 2: Map the credit rating to ordinal
df["credit_rating_rank"] = df["RD_CREDIT_RATING_GLOBAL"].map(credit_rating_map)

# Step 3: Create binary flag for "Not Rated"
df["was_not_rated"] = df["RD_CREDIT_RATING_GLOBAL"].eq("NR").astype(int)

#fill NA's present in SP_COMMON_DIV_DECLARED with 0 (as NA represents no dividend in this dataset)
company_metadata_df['SP_COMMON_DIV_DECLARED'] = company_metadata_df['SP_COMMON_DIV_DECLARED'].fillna(0)

"""#Cleaning Data

* Clean up any issues found in dataset

* Ensure everything is ordered chronologically (extra step against data leakage)

* Merge all of the datasets using the following logic:
  * The dialy data will serve as the main dataframe
  * For the static company level data, we will merge once and keep the information static and add it to each line level item
  * For the quarterly data, we will attach the 'last known' quarterly value to each row. This will be a 'last known' forward fill approach, only using past data

* One-hot encode or ordinal-encode categorical static vars
  * Variables we will have to encode:
    * Credit Rating
    * We need to embed entity ID so that we ensure that our model recognizes behavior at a firm specific level
      * EMBEDDING IS DONE AT THE MODEL CREATION STEP, SO DISREGARD DURING PREPROCCESSING

##Chronological Order
"""

#Ensure all of our datasets are in chronological order to ensure proper training and to ensure no data leakage


#Daily dataframe ordering
# Ensure datetime format
daily_master_df["date"] = pd.to_datetime(daily_master_df["date"])

# Sort by company and date
daily_master = daily_master_df.sort_values(by=["ticker", "date"]).reset_index(drop=True)

#Quarterly dataframe ordering
import pandas as pd

# Function to convert Quarter string to a quarter-end date
def convert_quarter_to_date(q):
    year = int(q[:4])
    quarter = int(q[-1])
    if quarter == 1:
        return pd.Timestamp(f"{year}-03-31")
    elif quarter == 2:
        return pd.Timestamp(f"{year}-06-30")
    elif quarter == 3:
        return pd.Timestamp(f"{year}-09-30")
    elif quarter == 4:
        return pd.Timestamp(f"{year}-12-31")
    else:
        return pd.NaT  # handle any malformed entries

# Add new column without modifying the original
quarterly_raw_df["Date"] = quarterly_raw_df["quarter"].apply(convert_quarter_to_date)

# Sort by ticker and new Date column
quarterly_raw_df = quarterly_raw_df.sort_values(by=["sp_entity_id", "Date"]).reset_index(drop=True)


#display the results of the quarterly dataframe
display(quarterly_raw_df.head(10))

# Pivot to wide format: metrics become columns, values filled in
pivoted_df = quarterly_raw_df.pivot_table(
    index=["sp_entity_id", "ticker", "quarter", "Date"],
    columns="metric",
    values="value",
    aggfunc="first"
).reset_index()

# Overwrite original
quarterly_raw_df = pivoted_df.copy()

# Remove column name for 'metric' if needed
quarterly_raw_df.columns.name = None

quarterly_raw_df.head(10)

"""##Merging into single, master dataframe"""

print("ğŸ“„ company_metadata_df columns:")
print(company_metadata_df.columns.tolist())

print("\nğŸ“„ daily_master_df columns:")
print(daily_master_df.columns.tolist())

print("\nğŸ“„ quarterly_raw_df columns:")
print(quarterly_raw_df.columns.tolist())

print("\nğŸ“„ yield_df columns:")
print(yield_df.columns.tolist())

"""What do we want in the main dataframe?

date, entity name, entity ID, ticker, credit rating rank, was not rated

close, market cap, open, volume

book value equity and net income

T10Y2Y
"""

yield_df.head(10)

import pandas as pd

# --- STEP 1: Prepare daily data ---
daily_cols = ["date", "sp_entity_id", "ticker", "close", "market_cap", "open", "volume"]
daily_df = daily_master_df[daily_cols].copy()
daily_df["date"] = pd.to_datetime(daily_df["date"])
daily_df["sp_entity_id"] = daily_df["sp_entity_id"].astype(str)

# --- STEP 2: Prepare metadata ---
meta_cols = {
    "SP_ENTITY_ID": "entity_ID",
    "SP_ENTITY_NAME": "entity_name",
    "SP_TICKER": "ticker",
    "credit_rating_rank": "credit_rating_rank",
    "was_not_rated": "was_not_rated"
}
meta_df = company_metadata_df[list(meta_cols.keys())].rename(columns=meta_cols)
meta_df["entity_ID"] = meta_df["entity_ID"].astype(str)

# Merge metadata into daily using entity_ID and ticker
daily_df = daily_df.merge(
    meta_df,
    left_on=["sp_entity_id", "ticker"],
    right_on=["entity_ID", "ticker"],
    how="left"
)

# --- STEP 3: Prepare quarterly data ---
quarterly_df = quarterly_raw_df.copy()
quarterly_df["Date"] = pd.to_datetime(quarterly_df["Date"])
quarterly_df["sp_entity_id"] = quarterly_df["sp_entity_id"].astype(str)

quarterly_df = quarterly_df[["Date", "sp_entity_id", "book_value_equity", "net_income"]]
quarterly_df = quarterly_df.sort_values(by=["Date", "sp_entity_id"]).reset_index(drop=True)
daily_df = daily_df.sort_values(by=["date", "sp_entity_id"]).reset_index(drop=True)

# Merge quarterly data using merge_asof
merged_df = pd.merge_asof(
    left=daily_df,
    right=quarterly_df,
    left_on="date",
    right_on="Date",
    by="sp_entity_id",
    direction="backward",
    tolerance=pd.Timedelta(days=100)
)
merged_df = merged_df.drop(columns=["Date"])

# --- STEP 4: Fix and merge yield curve spread ---
yield_df = yield_df.rename(columns={"observation_date": "date"})
yield_df["date"] = pd.to_datetime(yield_df["date"])
merged_df = merged_df.merge(yield_df[["date", "T10Y2Y"]], on="date", how="left")

# --- STEP 5: Select final columns ---
final_cols = [
    "date", "entity_name", "entity_ID", "ticker",
    "credit_rating_rank", "was_not_rated",
    "close", "market_cap", "open", "volume",
    "book_value_equity", "net_income",
    "T10Y2Y"
]

master_df = merged_df[final_cols].copy()
print("âœ… Final master_df shape:", master_df.shape)

master_df.head(10)

"""---

#Feature Engineering

##Features we want to engineer:


*   lagged close -- t-1, t-5, t-10, t-20
*   lagged return -- (same lag as above)
*   moving average (same lag)
*   daily average volatility score (we want the daily average volatility for each day, to do this we will take the Average Absolute Return across S&P 500 stocks for each day in the dataset
*   20 day rolling market volatility (we can calculate this using the daily average volatility)
*   RSI
*   Momentum indicators
*   Lagged treasury yield (we want to see the movement (is it increasing or decreasing in the past 20 days)

Lagged Close Prices
"""

# Lagged close prices per entity
lags = [1, 5, 10, 20]
for lag in lags:
    master_df[f"close_t-{lag}"] = master_df.groupby("entity_ID")["close"].shift(lag)

"""Lagged Returns"""

# Return = (close - open) / open for t, t-1, t-5, etc.
master_df["return_t"] = (master_df["close"] - master_df["open"]) / master_df["open"]

for lag in lags:
    close_lag = master_df.groupby("entity_ID")["close"].shift(lag)
    open_lag = master_df.groupby("entity_ID")["open"].shift(lag)
    master_df[f"return_t-{lag}"] = (close_lag - open_lag) / open_lag

"""Moving Averages"""

# Moving averages that include today
for window in [1, 5, 10, 20]:
    master_df[f"ma_close_{window}"] = (
        master_df.groupby("entity_ID")["close"]
        .transform(lambda x: x.rolling(window=window, min_periods=1).mean())
    )

"""Daily Average Market Volatility"""

# First calculate raw market-level return per stock for each day
master_df["abs_return_stock"] = ((master_df["close"] - master_df["open"]) / master_df["open"]).abs()
master_df["raw_return_stock"] = (master_df["close"] - master_df["open"]) / master_df["open"]

# Then aggregate per day to get average absolute return (volatility) and directional return
daily_vol_df = master_df.groupby("date").agg(
    avg_abs_return=("abs_return_stock", "mean"),
    avg_raw_return=("raw_return_stock", "mean")
).reset_index()

# Merge into master_df
master_df = master_df.merge(daily_vol_df, on="date", how="left")

"""Rolling Market Volatility"""

# Compute rolling volatility on the market-level avg_abs_return
for window in [1, 5, 10, 20]:
    master_df[f"rolling_market_vol_{window}d"] = (
        master_df.groupby("date")["avg_abs_return"]
        .transform(lambda x: x.rolling(window=window, min_periods=1).std())
    )

# Drop rolling_market_vol_1d since it's just a duplicate
master_df.drop(columns=['rolling_market_vol_1d'], inplace=True)

"""RSI 14 days

"""

!pip install ta

import ta

# Recalculate RSI per entity
def calc_rsi(group):
    return ta.momentum.RSIIndicator(close=group['close'], window=14).rsi()

# Apply per stock
master_df['rsi_14'] = master_df.groupby('entity_ID', group_keys=False).apply(calc_rsi)

"""Volume Features (lagged volume and Rolling Average)"""

# Lagged volume
for lag in [1, 5, 10, 20]:
    master_df[f"volume_t-{lag}"] = master_df.groupby("entity_ID")["volume"].shift(lag)

# 20-day rolling average volume
master_df["volume_ma_20"] = (
    master_df.groupby("entity_ID")["volume"]
    .transform(lambda x: x.rolling(window=20, min_periods=1).mean())
)

"""Treasury Yield Curve Change Over 20 days"""

# Calculate T10Y2Y_t - T10Y2Y_{t-20}
master_df["T10Y2Y_t-20"] = master_df["T10Y2Y"].shift(20)
master_df["T10Y2Y_delta_20d"] = master_df["T10Y2Y"] - master_df["T10Y2Y_t-20"]

"""Forward fill missing values from beginning of dataset"""

# Identify engineered features
engineered_cols = [col for col in master_df.columns if (
    any(key in col for key in ['_t-', '_delta', 'rolling_market_vol', 'rsi', 'return_', 'avg_', 'ma_close'])
)]

# Forward fill them per stock
master_df[engineered_cols] = master_df.groupby('entity_ID')[engineered_cols].ffill()

# ---- Overview ----
print(" master_df shape:", master_df.shape)
print("\n Column data types:")
print(master_df.dtypes)

# ---- Count of NaNs per column ----
print("\n NaN counts per column:")
nan_counts = master_df.isna().sum()
print(nan_counts[nan_counts > 0])

# ---- % of NaNs per column ----
print("\n Percent of NaNs per column:")
print((nan_counts / len(master_df) * 100).round(2))

# ---- Summary statistics for numerical columns ----
print("\n Summary statistics (numerical):")
display(master_df.describe().T)

# ---- View sample rows with any missing values ----
print("\n Sample rows with missing values:")
display(master_df[master_df.isna().any(axis=1)].head(10))

"""Handling NaN's"""

# Count how many NaNs each entity has in a key column
key_cols_to_check = ['close', 'open', 'volume', 'book_value_equity', 'net_income']

for col in key_cols_to_check:
    print(f"\n NaN count by entity for: {col}")
    display(
        master_df[master_df[col].isna()]
        .groupby("entity_ID")["date"]
        .count()
        .sort_values(ascending=False)
        .head(10)  # Top 10 worst
    )

# Ensure the data is sorted before filling
master_df = master_df.sort_values(by=['entity_ID', 'date'])


#  Forward-fill per entity
# --------------------------
cols_to_ffill_by_entity = ['open', 'book_value_equity', 'net_income']

for col in cols_to_ffill_by_entity:
    master_df[col] = master_df.groupby('entity_ID')[col].ffill()


#  Forward-fill macro features globally
# --------------------------
macro_cols_to_ffill = ['T10Y2Y', 'T10Y2Y_t-20', 'T10Y2Y_delta_20d']

for col in macro_cols_to_ffill:
    master_df[col] = master_df[col].ffill()


#  Drop remaining rows with NaNs
# --------------------------
# Remaining NaNs will mostly be from lagged/rolling features (first N rows per entity)
master_df = master_df.dropna().reset_index(drop=True)


#  Confirm NaNs are handled
# --------------------------
print("\n Remaining NaNs per column:")
print(master_df.isna().sum()[master_df.isna().sum() > 0])

print(f"\n Final shape of cleaned master_df: {master_df.shape}")

"""#Creating a new dataset with only volatile event windows

Volatility definition: 2 std. deviations from mean for each stock... drop to 1.5 if we don't get enough observations
* make sure we are looking at each stock's volatility as a %... and make sure we are looking at each company's movement individually. Potentially make it a feature (added column)



*   Number of volatile periods observed


periods used: 60 days prior to volatile event and 20 days after volatile event

##New Volatility Technique:

Computing average volatility over the whole time period per stock and then looking at events that are volatilie in comparison to that number to identify volate events
"""

# Calculate daily returns
master_df['daily_return'] = master_df.groupby('entity_ID')['close'].pct_change()

# Get the mean and std of daily return per stock
volatility_stats = (
    master_df.groupby('entity_ID')['daily_return']
    .agg(['mean', 'std'])
    .reset_index()
    .rename(columns={'mean': 'avg_return', 'std': 'return_std'})
)

# Merge stats into master_df
master_df = master_df.merge(volatility_stats, on='entity_ID', how='left')

# Compute z-score of each return relative to global std
master_df['return_z_score'] = (
    (master_df['daily_return'] - master_df['avg_return']) / master_df['return_std']
)

# Flag volatility shock if z-score exceeds 2 or -2
master_df['is_volatility_shock'] = master_df['return_z_score'].abs() > 2

# Filter to only rows with a volatility shock (new method)
vol_df_global = master_df[master_df['is_volatility_shock']].copy()

# Sort and reindex
vol_df_global = vol_df_global.sort_values(['entity_ID', 'date']).reset_index(drop=True)

# Flag shocks that are part of sequential streaks
vol_df_global['prev_date'] = vol_df_global.groupby('entity_ID')['date'].shift(1)
vol_df_global['day_diff'] = (vol_df_global['date'] - vol_df_global['prev_date']).dt.days
vol_df_global['new_streak'] = (vol_df_global['day_diff'] > 1) | (vol_df_global['day_diff'].isna())
vol_df_global['streak_id'] = vol_df_global.groupby('entity_ID')['new_streak'].cumsum()

# Keep only last date in each streak
vol_df_global = vol_df_global.groupby(['entity_ID', 'streak_id']).tail(1).reset_index(drop=True)

print("Total shocks (global):", master_df['is_volatility_shock'].sum())
print("Non-redundant shocks (global):", len(vol_df_global))
print("Unique companies with shocks:", vol_df_global['entity_ID'].nunique())

import matplotlib.pyplot as plt

vol_df_global['month'] = vol_df_global['date'].dt.month
vol_df_global['quarter'] = vol_df_global['date'].dt.quarter

fig, axs = plt.subplots(1, 2, figsize=(14, 4))

vol_df_global['month'].value_counts().sort_index().plot(kind='bar', ax=axs[0])
axs[0].set_title("Shocks by Month (Global Method)")
axs[0].set_xlabel("Month")
axs[0].set_ylabel("Count")

vol_df_global['quarter'].value_counts().sort_index().plot(kind='bar', ax=axs[1])
axs[1].set_title("Shocks by Quarter (Global Method)")
axs[1].set_xlabel("Quarter")
axs[1].set_ylabel("Count")

plt.tight_layout()
plt.show()

# Step 1: Count shocks per company
shock_counts = vol_df_global['entity_ID'].value_counts().reset_index()
shock_counts.columns = ['entity_ID', 'shock_count']  # rename columns clearly

# Step 2: Reset vol_df index and make sure 'entity_ID' and 'entity_name' are columns
vol_df_global_reset = vol_df_global.reset_index(drop=True)

# Step 3: Merge in entity_name using drop_duplicates
company_lookup = vol_df_global_reset[['entity_ID', 'entity_name']].drop_duplicates()
top_shocks = shock_counts.merge(company_lookup, on='entity_ID', how='left')

# Step 4: Display top 25
top_25_stocks = top_shocks.head(25)
top_25_stocks

most_volatile_days = (
    vol_df_global.groupby('date').size().reset_index(name='num_shocks')
    .sort_values('num_shocks', ascending=False)
)
display(most_volatile_days.head(10))

vol_df_global['shock_polarity'] = vol_df_global['daily_return'].apply(lambda x: 'positive' if x > 0 else 'negative')
display(vol_df_global['shock_polarity'].value_counts())

"""Examining New Dataframe"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Use vol_df_global directly
vol_df = vol_df_global.copy()

# 1. Total number of volatility shock windows
num_windows = len(vol_df)
print(f"Total volatility shock windows: {num_windows}")

# 2. Number of unique companies that experienced at least one shock
unique_companies = vol_df['entity_ID'].nunique()
print(f"Number of unique companies with shocks: {unique_companies}")

# 3. Number of shocks per company
shocks_per_company = (
    vol_df['entity_ID']
    .value_counts()
    .rename_axis('entity_ID')
    .reset_index(name='shock_count')
)

# Merge entity_name into shocks_per_company
shocks_per_company = shocks_per_company.merge(
    vol_df[['entity_ID', 'entity_name']].drop_duplicates(),
    on='entity_ID',
    how='left'
)

# View top 10 with names
print("\nShocks per company (top 10):")
print(shocks_per_company.head(10))

# 4. Distribution of shocks per day
shocks_per_day = (
    vol_df['date']
    .value_counts()
    .rename_axis('date')
    .reset_index(name='num_shocks')
)

print("\nShocks per day summary:")
print(shocks_per_day['num_shocks'].describe())

# 5. Plot distribution of shocks per day
plt.figure(figsize=(12, 5))
sns.histplot(shocks_per_day['num_shocks'], bins=30, kde=False)
plt.title('Distribution of Number of Volatility Shocks Per Day (Global Method)')
plt.xlabel('Number of Shocks')
plt.ylabel('Frequency (Days)')
plt.tight_layout()
plt.show()

"""Days with the largest, individual stock moves"""

# Get absolute return per stock
master_df['abs_return'] = master_df['daily_return'].abs()

# Calculate % move from open to close
master_df['pct_move'] = ((master_df['close'] - master_df['open']) / master_df['open']) * 100

# Find top movers across all dates
top_moves = (
    master_df.sort_values('abs_return', ascending=False)
    [['entity_ID', 'entity_name', 'date', 'open', 'close', 'daily_return', 'abs_return', 'pct_move']]
    .dropna()
    .head(20)
)

# Display the result
print(top_moves)

"""The most volatile days in the period"""

# Get the correct shock counts per day
most_volatile_days = (
    vol_df_global.groupby('date')
    .size()
    .reset_index(name='num_shocks')  # Correctly names the count column
    .sort_values('num_shocks', ascending=False)
)

# Display the top 20 most volatile days
display(most_volatile_days.head(20))

"""Stock Polarity"""

import matplotlib.pyplot as plt
import seaborn as sns

# Classify polarity for shocks in vol_df_global
vol_df_global['shock_polarity'] = vol_df_global['daily_return'].apply(
    lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral')
)

# Polarity summary
polarity_counts = vol_df_global['shock_polarity'].value_counts().reset_index()
polarity_counts.columns = ['shock_polarity', 'count']
display(polarity_counts)

# Visualization
plt.figure(figsize=(6, 4))
sns.barplot(data=polarity_counts, x='shock_polarity', y='count')
plt.title('Shock Polarity Counts (vol_df_global)')
plt.xlabel('Shock Polarity')
plt.ylabel('Count')
plt.tight_layout()
plt.show()

"""Visualizing shocks over time"""

import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Count number of shocks per day (returns Series with index=date, value=count)
shock_counts = vol_df_global['date'].value_counts().sort_index()

# Step 2: Convert to DataFrame and rename columns explicitly
most_volatile_days = shock_counts.reset_index()
most_volatile_days.columns = ['date', 'num_shocks']

# Step 3: Ensure 'date' is datetime (in case it's not)
most_volatile_days['date'] = pd.to_datetime(most_volatile_days['date'])

# Step 4: Plot
plt.figure(figsize=(14, 5))
sns.lineplot(data=most_volatile_days, x='date', y='num_shocks')
plt.title('Number of Volatility Shocks per Day (vol_df_global)')
plt.xlabel('Date')
plt.ylabel('Number of Stocks with Volatility Shock')
plt.tight_layout()
plt.show()

"""Same graph, now with 7 day rolling average line and highlighted most volatile days"""

# Copy to avoid modifying original
volatility_plot_df = most_volatile_days.copy()

# Add 7-day rolling average column
volatility_plot_df['rolling_avg'] = volatility_plot_df['num_shocks'].rolling(window=7).mean()

# Identify the top 10 most volatile days
top10_days = volatility_plot_df.sort_values('num_shocks', ascending=False).head(10)

# Plot with both lines and highlight points
plt.figure(figsize=(14, 5))
sns.lineplot(data=volatility_plot_df, x='date', y='num_shocks', label='Daily Shocks')
sns.lineplot(data=volatility_plot_df, x='date', y='rolling_avg', label='7-Day Rolling Avg', linestyle='--')

# Highlight top 10 days
plt.scatter(top10_days['date'], top10_days['num_shocks'], color='red', zorder=5, label='Top 10 Days')
for _, row in top10_days.iterrows():
    plt.text(row['date'], row['num_shocks'] + 1, row['date'].strftime('%Y-%m-%d'), rotation=45, ha='right', fontsize=8)

plt.title('Volatility Shocks per Day with 7-Day Rolling Average')
plt.xlabel('Date')
plt.ylabel('Number of Stocks with Volatility Shock')
plt.legend()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Ensure copy for plotting
vol_df_global_plot = vol_df_global.copy()

# Add polarity if not present
if 'shock_polarity' not in vol_df_global_plot.columns:
    vol_df_global_plot['shock_polarity'] = vol_df_global_plot['daily_return'].apply(
        lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral')
    )

# Group by date and polarity
daily_polar_shocks = (
    vol_df_global_plot.groupby(['date', 'shock_polarity'])
    .size()
    .unstack(fill_value=0)
    .rename(columns={'positive': 'num_pos_shocks', 'negative': 'num_neg_shocks'})
    .reset_index()
)

# Add total column (not plotted here, just for sorting)
daily_polar_shocks['total'] = (
    daily_polar_shocks.get('num_pos_shocks', 0) + daily_polar_shocks.get('num_neg_shocks', 0)
)

# Identify top 5 days with most positive shocks
top5_pos = daily_polar_shocks.nlargest(5, 'num_pos_shocks')

# Identify top 5 days with most negative shocks
top5_neg = daily_polar_shocks.nlargest(5, 'num_neg_shocks')

# Plot
plt.figure(figsize=(14, 5))
sns.lineplot(data=daily_polar_shocks, x='date', y='num_pos_shocks', label='Positive Shocks', color='green')
sns.lineplot(data=daily_polar_shocks, x='date', y='num_neg_shocks', label='Negative Shocks', color='red')

# Add red blips and annotations for top negative shock days
plt.scatter(top5_neg['date'], top5_neg['num_neg_shocks'], color='red', s=60, zorder=5, label='Top 5 Negative Days')
for _, row in top5_neg.iterrows():
    plt.text(row['date'], row['num_neg_shocks'] + 1, row['date'].strftime('%Y-%m-%d'),
             rotation=45, ha='right', fontsize=8, color='red')

# Add green blips and annotations for top positive shock days
plt.scatter(top5_pos['date'], top5_pos['num_pos_shocks'], color='green', s=60, zorder=5, label='Top 5 Positive Days')
for _, row in top5_pos.iterrows():
    plt.text(row['date'], row['num_pos_shocks'] + 1, row['date'].strftime('%Y-%m-%d'),
             rotation=45, ha='right', fontsize=8, color='green')

plt.title('Top Positive and Negative Volatility Shocks per Day')
plt.xlabel('Date')
plt.ylabel('Number of Stocks with Shock')
plt.legend()
plt.tight_layout()
plt.show()

"""Stacked Polarity chart"""

import matplotlib.pyplot as plt

# Use same polarity dataframe
daily_polar_shocks = (
    vol_df_global.groupby(['date', 'shock_polarity'])
    .size()
    .unstack(fill_value=0)
    .rename(columns={'positive': 'num_pos_shocks', 'negative': 'num_neg_shocks'})
    .reset_index()
)

# Plot as stacked area
plt.figure(figsize=(14, 5))
plt.stackplot(
    daily_polar_shocks['date'],
    daily_polar_shocks['num_pos_shocks'],
    daily_polar_shocks['num_neg_shocks'],
    labels=['Positive Shocks', 'Negative Shocks'],
    colors=['green', 'red']
)
plt.title('Stacked Area Chart of Positive and Negative Shocks Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Shocks')
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

# Add net shock score to daily shock data
daily_polar_shocks['net_shock_score'] = (
    daily_polar_shocks.get('num_pos_shocks', 0) - daily_polar_shocks.get('num_neg_shocks', 0)
)

# Plot net shock score
plt.figure(figsize=(14, 5))
sns.lineplot(data=daily_polar_shocks, x='date', y='net_shock_score', color='purple')
plt.axhline(0, color='black', linestyle='--', linewidth=1)
plt.title('Net Shock Score Over Time (Positive - Negative)')
plt.xlabel('Date')
plt.ylabel('Net Shock Score')
plt.tight_layout()
plt.show()

"""Lets look at the most volatile days and make some sense of them"""

# Show top 10 most volatile days with number of shocks
print("Top 10 most volatile days:")
display(top10_days.sort_values('num_shocks', ascending=False).reset_index(drop=True))

"""# ***vol_df_global is the dataset we will use in modelling***

#Creating target variables

Creating columns:

close_t_plus_1

target_return_1d

close_t_plus_5

target_return_5d

close_t_plus_20

target_return_20d
"""

# Generate targets for 1, 5, and 20 trading days ahead
target_horizons = [1, 5, 20]
for horizon in target_horizons:
    master_df[f'target_return_{horizon}d'] = (
        master_df.groupby('entity_ID')['close'].shift(-horizon) - master_df['close']
    ) / master_df['close']

# Keep only necessary columns to merge
target_cols = ['entity_ID', 'date'] + [f'target_return_{h}d' for h in target_horizons]
targets_df = master_df[target_cols]

# Merge into vol_df_global safely
vol_df_global = vol_df_global.merge(targets_df, on=['entity_ID', 'date'], how='left')

# Check for NaNs (only expected near end of data)
missing_targets_summary = vol_df_global[
    [f'target_return_{h}d' for h in target_horizons]
].isna().sum()

print(" Missing values in target columns (only end-of-data is expected):")
print(missing_targets_summary)

"""#Windowing and Train/Test Splitting


*   We are going to deploy expanding windows

* The prediction periods will be 1 trading day, 5 trading days, and 20 trading days so we will keep this in mind when creating our split

* Our data is a total of 24 months, or X days of
  * our training set will be 18 months, or X
    * months 1-18
  * our validation set will be 2 months, or X days
    * months 19-20
  * our test set will be 4 months, or X days
    * months 21-24



"""

# Convert to datetime just in case
vol_df_global['date'] = pd.to_datetime(vol_df_global['date'])

# Define cutoff dates
train_start = pd.to_datetime("2023-01-01")
train_end = pd.to_datetime("2024-06-30")

val_start = pd.to_datetime("2024-07-01")
val_end = pd.to_datetime("2024-08-31")

test_start = pd.to_datetime("2024-09-01")
test_end = pd.to_datetime("2024-12-31")

# Create splits
train_df = vol_df_global[(vol_df_global['date'] >= train_start) & (vol_df_global['date'] <= train_end)].copy()
val_df = vol_df_global[(vol_df_global['date'] >= val_start) & (vol_df_global['date'] <= val_end)].copy()
test_df = vol_df_global[(vol_df_global['date'] >= test_start) & (vol_df_global['date'] <= test_end)].copy()

# Confirm splits
print("Train set:", train_df['date'].min(), "to", train_df['date'].max(), "| Rows:", len(train_df))
print("Val set:", val_df['date'].min(), "to", val_df['date'].max(), "| Rows:", len(val_df))
print("Test set:", test_df['date'].min(), "to", test_df['date'].max(), "| Rows:", len(test_df))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure 'date' is datetime
vol_df_global['date'] = pd.to_datetime(vol_df_global['date'])

# Add a 'month' column for grouping
vol_df_global['month'] = vol_df_global['date'].dt.to_period('M')

# Count number of shocks per month
monthly_counts = vol_df_global.groupby('month').size().reset_index(name='num_shocks')
monthly_counts['month'] = monthly_counts['month'].dt.to_timestamp()

# Plot
plt.figure(figsize=(12, 6))
sns.barplot(data=monthly_counts, x='month', y='num_shocks', color='skyblue')
plt.xticks(rotation=45)
plt.title('Number of Volatility Shocks Per Month')
plt.xlabel('Month')
plt.ylabel('Number of Shocks')
plt.tight_layout()
plt.show()

"""#Scale Master DF & Train/Val/Test DF's"""

print(vol_df_global.columns.tolist())

"""Commented out because scaling is not required for XGBoost and CatBoost Models"""

# from sklearn.preprocessing import StandardScaler

# # Only needed if you're prepping for an LSTM or another NN
# # Define columns NOT to scale
# exclude_cols = [
#     'entity_ID', 'entity_ID_encoded', 'date', 'shock_date', 'shock_entity_ID',
#     'target_return_1d', 'target_return_5d', 'target_return_20d',
#     'is_volatility_shock', 'is_vol_shock_day', 'shock_polarity',
#     'prev_date', 'day_diff', 'new_streak', 'streak_id'
# ]

# # Get all numeric columns across your master set
# numeric_cols = master_df.select_dtypes(include=['float64', 'int64']).columns.tolist()

# # Remove excluded ones
# features_to_scale = [col for col in numeric_cols if col not in exclude_cols]

# # Fit scaler on training data only
# scaler = StandardScaler()
# scaler.fit(train_df[features_to_scale])

# # Apply scaling to train/val/test ONLY IF you're using an LSTM/NN
# for df in [train_df, val_df, test_df]:
#     df[features_to_scale] = scaler.transform(df[features_to_scale])

"""#Model Building

Create Baseline Model that can test "Random Choice"

*   We will use a mean predictor (predicts the mean return seen in the training set)
*   Choose performance metrics: MSE / MAE

**Mean Absolute Error (MAE)**
What it measures: Average of the absolute differences between predicted and actual values.

Interpretability: Very interpretable â€” â€œon average, weâ€™re off by X% or X price units.â€

Good for: When you care equally about all errors regardless of direction or magnitude.

**Root Mean Squared Error (RMSE)**
What it measures: Square root of the average squared difference.

Interpretability: Penalizes larger errors more than MAE, which is useful if large prediction errors are especially harmful.

Good for: When you want to penalize extreme mispredictions.


 **RÂ² Score (Coefficient of Determination)**
What it measures: Proportion of variance explained by your model.

Value range: 1 (perfect) to negative infinity (worse than baseline).

Caution: Not always meaningful with noisy financial data, but can help compare models.
"""

##Baseline Model

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define target columns
target_cols = ['target_return_1d', 'target_return_5d', 'target_return_20d']

# Step 1: Compute mean of target returns from training set
mean_predictions = train_df[target_cols].mean()

print("Mean predictions from training set:")
for col in target_cols:
    print(f"{col}: {mean_predictions[col]:.6f}")

# Step 2: Evaluate on validation set
results = []

for col in target_cols:
    y_true = val_df[col].dropna()
    y_pred = np.full_like(y_true, fill_value=mean_predictions[col], dtype=np.float64)

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Compatible RMSE
    r2 = r2_score(y_true, y_pred)

    results.append({
        'Target': col,
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2
    })

# Convert to DataFrame for nice display
results_df = pd.DataFrame(results)
print("\nValidation Metrics:")
display(results_df)

"""Baseline Model Expalined:

The model doesn't actually "learn" anything from the features.
It simply predicts the mean return from the training set for every example in the validation set.

Evaluation of Results:

1. RÂ² < 0 for all targets
This confirms the baseline is no better than guessing the mean of the validation set.

Your future models need only beat this simple approach to add value.

2. MAE and RMSE are non-trivial
These errors tell you how much a naive guess would deviate from true outcomes.

For instance, the 20-day MAE is ~5.7% return â€” this means your baseline predictions are off by about 5.7% on average.

#Feature Selection
"""

# Exclude these from model input
exclude_cols = [
    'entity_ID', 'entity_ID_encoded', 'date', 'shock_date', 'shock_entity_ID',
    'target_return_1d', 'target_return_5d', 'target_return_20d',
    'is_volatility_shock', 'is_vol_shock_day', 'shock_polarity',
    'prev_date', 'day_diff', 'new_streak', 'streak_id'
]

# Define feature columns
feature_cols = [col for col in train_df.columns if col not in exclude_cols]

# Target for now (you can switch between 1d, 5d, or 20d later)
target_col = 'target_return_5d'

X_train = train_df[feature_cols]
y_train = train_df[target_col]

"""Feature importance using CatBoost"""

from sklearn.model_selection import train_test_split

# Define columns not usable as features
drop_cols = [
    'date', 'entity_ID', 'entity_name', 'ticker', 'shock_polarity',
    'prev_date', 'day_diff', 'new_streak', 'streak_id',
    'is_volatility_shock', 'target_return_1d', 'target_return_5d', 'target_return_20d'
]

# Choose your target (customize if needed)
target_col = 'target_return_5d'

# Features and target
X_train = train_df.drop(columns=drop_cols, errors='ignore')
y_train = train_df[target_col]

X_val = val_df.drop(columns=drop_cols, errors='ignore')
y_val = val_df[target_col]

!pip install catboost

from catboost import CatBoostRegressor, Pool
import matplotlib.pyplot as plt
import pandas as pd

# Initialize and fit
cat_model = CatBoostRegressor(verbose=0, random_state=42)
cat_model.fit(X_train, y_train)

# Get and sort importances
feature_importance = pd.Series(cat_model.get_feature_importance(), index=X_train.columns)
feature_importance = feature_importance.sort_values(ascending=False)

# Plot
plt.figure(figsize=(10, 6))
feature_importance.head(20).plot(kind='barh')
plt.title("CatBoost Feature Importance (Top 20)")
plt.xlabel("Importance")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""Feature importance using XGBoost"""

import xgboost as xgb

xgb_model = xgb.XGBRegressor(random_state=42, n_estimators=100)
xgb_model.fit(X_train, y_train)

xgb_importance = pd.Series(xgb_model.feature_importances_, index=X_train.columns)
xgb_importance = xgb_importance.sort_values(ascending=False)

# Plot
plt.figure(figsize=(10, 6))
xgb_importance.head(20).plot(kind='barh', color='orange')
plt.title("XGBoost Feature Importance (Top 20)")
plt.xlabel("Importance")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""Permutation Importance:"""

from sklearn.inspection import permutation_importance

perm_result = permutation_importance(cat_model, X_val, y_val, n_repeats=10, random_state=42)
perm_importance = pd.Series(perm_result.importances_mean, index=X_val.columns).sort_values(ascending=False)

plt.figure(figsize=(10, 6))
perm_importance.head(20).plot(kind='barh', color='green')
plt.title("Permutation Feature Importance (Top 20, CatBoost)")
plt.xlabel("Importance Decrease")
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

"""Corellation of Features"""

import seaborn as sns
import matplotlib.pyplot as plt

# Only include numerical features used in training
corr_features = X_train.select_dtypes(include=['float64', 'int64'])

# Compute correlation matrix
corr_matrix = corr_features.corr()

# Visualize the upper triangle
plt.figure(figsize=(16, 12))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)
plt.title('Correlation Matrix of Training Features')
plt.show()

# Find highly correlated pairs
def get_high_corr_pairs(corr_matrix, threshold=0.85):
    high_corr = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > threshold:
                col1 = corr_matrix.columns[i]
                col2 = corr_matrix.columns[j]
                corr_val = corr_matrix.iloc[i, j]
                high_corr.append((col1, col2, corr_val))
    return sorted(high_corr, key=lambda x: -abs(x[2]))

high_corr_pairs = get_high_corr_pairs(corr_matrix)

# Display top correlated pairs
import pandas as pd
high_corr_df = pd.DataFrame(high_corr_pairs, columns=["Feature 1", "Feature 2", "Correlation"])
display(high_corr_df.head(20))

"""SHAP"""

!pip install shap

# ğŸ“š Step 2: Import libraries
import shap
import matplotlib.pyplot as plt
from catboost import CatBoostRegressor

# ğŸ§¼ Step 3: Define final feature list
final_features = [
    'month', 'close', 'return_t', 'volume', 'rolling_market_vol_10d', 'T10Y2Y_t-20',
    'avg_abs_return', 'avg_raw_return', 'T10Y2Y', 'return_t-5', 'return_t-1',
    'avg_return', 'return_std', 'rsi_14', 'daily_return', 'net_income',
    'return_t-10', 'abs_return_stock', 'book_value_equity', 'market_cap'
]

X_train_final = X_train[final_features]
X_val_final = X_val[final_features]

# Step 4: Train CatBoost
cat_model = CatBoostRegressor(verbose=0)
cat_model.fit(X_train_final, y_train)

# Compute SHAP values
explainer = shap.Explainer(cat_model)
shap_values = explainer(X_val_final)

# SHAP Beeswarm Plot (summary of top features)
shap.plots.beeswarm(shap_values, max_display=20)

# SHAP Bar Plot (mean absolute impact)
shap.plots.bar(shap_values, max_display=20)

# SHAP Waterfall Plot (for first prediction)
shap.plots.waterfall(shap_values[0])

y_train.describe()

import matplotlib.pyplot as plt
plt.hist(y_train, bins=100)
plt.title("Distribution of target_return_1d")
plt.show()

"""Final Feature List"""

selected_features = [
    "month", "close", "return_t", "volume", "rolling_market_vol_10d", "T10Y2Y_t-20",
    "avg_abs_return", "avg_raw_return", "T10Y2Y", "return_t-5", "return_t-1",
    "avg_return", "return_std", "rsi_14", "daily_return", "net_income",
    "return_t-10", "abs_return_stock", "book_value_equity", "market_cap"
]

# Select only the finalized features
X_train_final = X_train[selected_features].copy()
X_val_final = X_val[selected_features].copy()

# If you're using a test set:
# X_test_final = X_test[selected_features].copy()

# Check for nulls
print("Missing values in training set:")
print(X_train_final.isnull().sum().sort_values(ascending=False))

# Optional: Fill or drop NaNs depending on your modeling strategy
# X_train_final = X_train_final.fillna(0)
# X_val_final = X_val_final.fillna(0)

"""#Model Building

Random Forest
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Feature set
selected_features = [
    "month", "close", "return_t", "volume", "rolling_market_vol_10d", "T10Y2Y_t-20",
    "avg_abs_return", "avg_raw_return", "T10Y2Y", "return_t-5", "return_t-1",
    "avg_return", "return_std", "rsi_14", "daily_return", "net_income",
    "return_t-10", "abs_return_stock", "book_value_equity", "market_cap"
]

# Targets to loop through
target_list = ["target_return_1d", "target_return_5d", "target_return_20d"]

# Train and evaluate a Random Forest for each target
for target_col in target_list:
    print(f"\n--- Training Random Forest for {target_col} ---")

    # Prepare X and y
    X_train = train_df[selected_features]
    y_train = train_df[target_col]

    X_val = val_df[selected_features]
    y_val = val_df[target_col]

    # Train model
    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
    rf_model.fit(X_train, y_train)

    # Predict
    y_pred = rf_model.predict(X_val)

    # Evaluate
    mse = mean_squared_error(y_val, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_val, y_pred)

    print(f"RMSE: {rmse:.6f}")
    print(f"RÂ²: {r2:.6f}")

"""XGBoost"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

# Assume X_train, X_val, and vol_df_global are already defined
selected_features = [
    "month", "close", "return_t", "volume", "rolling_market_vol_10d", "T10Y2Y_t-20",
    "avg_abs_return", "avg_raw_return", "T10Y2Y", "return_t-5", "return_t-1",
    "avg_return", "return_std", "rsi_14", "daily_return", "net_income",
    "return_t-10", "abs_return_stock", "book_value_equity", "market_cap"
]

target_cols = ['target_return_1d', 'target_return_5d', 'target_return_20d']
full_df = vol_df_global  # Adjust if your X_train/X_val were sourced elsewhere

for target_col in target_cols:
    print(f"\n--- Training XGBoost for {target_col} ---")

    X_train_target = X_train[selected_features]
    X_val_target = X_val[selected_features]

    y_train_target = full_df.loc[X_train.index, target_col]
    y_val_target = full_df.loc[X_val.index, target_col]

    model = XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.05, random_state=42)
    model.fit(X_train_target, y_train_target)

    y_pred_val = model.predict(X_val_target)

    # Evaluation metrics
    rmse = mean_squared_error(y_val_target, y_pred_val) ** 0.5  # Manual RMSE
    mae = mean_absolute_error(y_val_target, y_pred_val)
    r2 = r2_score(y_val_target, y_pred_val)
    directional_accuracy = np.mean(np.sign(y_val_target) == np.sign(y_pred_val))

    print(f"MAE: {mae:.6f} | RMSE: {rmse:.6f} | RÂ²: {r2:.6f} | Directional Accuracy: {directional_accuracy:.2%}")

"""CatBoost"""

from catboost import CatBoostRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Define targets and features
targets = ['target_return_1d', 'target_return_5d', 'target_return_20d']
selected_features = [
    "month", "close", "return_t", "volume", "rolling_market_vol_10d", "T10Y2Y_t-20",
    "avg_abs_return", "avg_raw_return", "T10Y2Y", "return_t-5", "return_t-1",
    "avg_return", "return_std", "rsi_14", "daily_return", "net_income",
    "return_t-10", "abs_return_stock", "book_value_equity", "market_cap"
]

# Prepare feature matrices
X_train = train_df[selected_features]
X_val = val_df[selected_features]

# Loop through each target
for target_col in targets:
    print(f"\n--- Training CatBoost for {target_col} ---")

    # Target vectors from the correct splits
    y_train_target = train_df[target_col]
    y_val_target = val_df[target_col]

    # Train CatBoost
    model = CatBoostRegressor(verbose=0, random_state=42)
    model.fit(X_train, y_train_target)

    # Predict
    y_pred_val = model.predict(X_val)

    # Metrics
    mae = mean_absolute_error(y_val_target, y_pred_val)
    rmse = np.sqrt(mean_squared_error(y_val_target, y_pred_val))
    r2 = r2_score(y_val_target, y_pred_val)

    # Directional Accuracy
    true_direction = y_val_target >= 0
    pred_direction = y_pred_val >= 0
    directional_accuracy = (true_direction == pred_direction).mean() * 100

    print(f"MAE: {mae:.6f} | RMSE: {rmse:.6f} | RÂ²: {r2:.6f} | Directional Accuracy: {directional_accuracy:.2f}%")

"""# XGBoost Final Model Creation

##Hyperparameter Tuning

Analyzing Target Distribution
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set target columns
target_cols = ['target_return_1d', 'target_return_5d', 'target_return_20d']

for target in target_cols:
    data = vol_df_global[target].dropna()

    print(f"\n--- {target} ---")
    print(f"Mean: {data.mean():.6f}, Std: {data.std():.6f}, Min: {data.min():.6f}, Max: {data.max():.6f}")
    print(f"Skew: {data.skew():.2f}, Kurtosis: {data.kurtosis():.2f}")

    # Histogram with KDE
    plt.figure(figsize=(7, 4))
    sns.histplot(data, bins=100, kde=True)
    plt.title(f"Distribution of {target}")
    plt.xlabel("Return")
    plt.ylabel("Frequency")
    plt.grid(True)
    plt.show()

    # Directional imbalance
    pct_positive = (data > 0).mean() * 100
    pct_negative = (data < 0).mean() * 100
    print(f"Positive Returns: {pct_positive:.2f}% | Negative Returns: {pct_negative:.2f}%")

X_train_target = X_train_target.astype('float32')

from xgboost import XGBRegressor
from sklearn.model_selection import ParameterSampler, GroupKFold
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd

# -----------------------------------------------
# 1. Define target and features
# -----------------------------------------------
target_col = 'target_return_5d'

selected_features = [
    "month", "close", "return_t", "volume", "rolling_market_vol_10d",
    "T10Y2Y_t-20", "avg_abs_return", "avg_raw_return", "T10Y2Y", "return_t-5",
    "return_t-1", "avg_return", "return_std", "rsi_14", "daily_return",
    "net_income", "return_t-10", "abs_return_stock", "book_value_equity",
    "market_cap"
]

X_train_target = X_train[selected_features].copy()
y_train_target = vol_df_global.loc[X_train.index, target_col].copy()

# Drop NaNs in both X and y
valid_idx = X_train_target.dropna().index.intersection(y_train_target.dropna().index)
X_train_target = X_train_target.loc[valid_idx]
y_train_target = y_train_target.loc[valid_idx]

# Ensure float32
X_train_target = X_train_target.astype(np.float32)
y_train_target = y_train_target.astype(np.float32)

# -----------------------------------------------
# 2. Setup groups for GroupKFold
# -----------------------------------------------
# You should define this based on your use case, here we assume grouping by entity
if 'entity_ID' not in vol_df_global.columns:
    raise KeyError("Missing 'entity_ID' in vol_df_global for grouping.")

groups = vol_df_global.loc[valid_idx, 'entity_ID']
assert not groups.isnull().any(), "NaNs in group labels!"

# -----------------------------------------------
# 3. Define param grid and sampler
# -----------------------------------------------
param_grid = {
    'n_estimators': list(range(100, 501, 100)),
    'max_depth': list(range(3, 8)),
    'learning_rate': np.linspace(0.01, 0.15, 5),
    'subsample': np.linspace(0.7, 1.0, 4),
    'colsample_bytree': np.linspace(0.7, 1.0, 4),
    'gamma': np.linspace(0, 2, 3),
    'reg_alpha': np.linspace(0, 0.5, 3),
    'reg_lambda': np.linspace(0.5, 1.5, 3)
}

param_list = list(ParameterSampler(param_grid, n_iter=10, random_state=42))

# -----------------------------------------------
# 4. Manual hyperparameter tuning with GroupKFold
# -----------------------------------------------
cv = GroupKFold(n_splits=3)
results = []

for i, params in enumerate(param_list):
    model = XGBRegressor(random_state=42, **params)
    rmses = []
    try:
        for train_idx, val_idx in cv.split(X_train_target, y_train_target, groups=groups):
            X_train_cv, X_val_cv = X_train_target.iloc[train_idx], X_train_target.iloc[val_idx]
            y_train_cv, y_val_cv = y_train_target.iloc[train_idx], y_train_target.iloc[val_idx]

            model.fit(X_train_cv, y_train_cv)
            preds = model.predict(X_val_cv)

            rmse = np.sqrt(mean_squared_error(y_val_cv, preds))
            rmses.append(rmse)

        avg_rmse = np.mean(rmses)
        results.append((avg_rmse, params))
        print(f"[{i+1}/{len(param_list)}] âœ… RMSE: {avg_rmse:.6f} | Params: {params}")
    except Exception as e:
        print(f"[{i+1}/{len(param_list)}] âŒ Error: {e}")

# -----------------------------------------------
# 5. Report best parameters
# -----------------------------------------------
if results:
    best_rmse, best_params = sorted(results, key=lambda x: x[0])[0]
    print("\nğŸ¯ Best RMSE:", best_rmse)
    print("âœ… Best Parameters:", best_params)
else:
    print("\nâš ï¸ No successful parameter configurations found.")

from sklearn.model_selection import GroupKFold, RandomizedSearchCV
from xgboost import XGBRegressor
import numpy as np
from scipy.stats import uniform, randint

xgb = XGBRegressor(objective='reg:squarederror', random_state=42)

param_distributions = {
    "n_estimators": randint(100, 1000),
    "max_depth": randint(3, 10),
    "learning_rate": uniform(0.01, 0.2),  # 0.01 to 0.21
    "subsample": uniform(0.6, 0.4),       # 0.6 to 1.0
    "colsample_bytree": uniform(0.6, 0.4),
    "gamma": uniform(0, 5),
    "reg_alpha": uniform(0, 1),
    "reg_lambda": uniform(0, 2),
    # Optional additions:
    # "min_child_weight": randint(1, 10),
    # "scale_pos_weight": uniform(0.5, 2)
}

gkf = GroupKFold(n_splits=3)

search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_distributions,
    n_iter=100,  # robust size
    scoring="neg_root_mean_squared_error",
    cv=gkf,
    n_jobs=-1,
    verbose=2,
    random_state=42,
)

search.fit(X_train_target, y_train_target, groups=groups)

#  Use known-good feature list
X_cols = X_val_final.columns.tolist()

#  Initialize dictionaries
X_train_targets, y_train_targets = {}, {}
X_val_targets, y_val_targets = {}, {}

#  Use existing train_df and val_df
for target in target_list:
    # Training split (drop NaNs for that target)
    train_valid = train_df[target].notna()
    X_train_targets[target] = train_df.loc[train_valid, X_cols]
    y_train_targets[target] = train_df.loc[train_valid, target]

    # Validation split
    val_valid = val_df[target].notna()
    X_val_targets[target] = val_df.loc[val_valid, X_cols]
    y_val_targets[target] = val_df.loc[val_valid, target]

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, accuracy_score
import numpy as np

# Store evaluation results
eval_results = {}

for target in target_list:
    print(f"\n Evaluating target: {target}")

    X_train = X_train_targets[target]
    y_train = y_train_targets[target]
    X_val = X_val_targets[target]
    y_val = y_val_targets[target]

    # Initialize model with best params
    model = XGBRegressor(**best_params, random_state=42, verbosity=0)
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_val)

    # Evaluate RMSE manually
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))

    # Evaluate Directional Accuracy
    direction_true = np.sign(y_val)
    direction_pred = np.sign(y_pred)
    directional_accuracy = accuracy_score(direction_true, direction_pred)

    # Store results
    eval_results[target] = {
        'RMSE': rmse,
        'Directional Accuracy': directional_accuracy
    }

    # Print results
    print(f" RMSE: {rmse:.5f}")
    print(f" Directional Accuracy: {directional_accuracy:.2%}")

print("\n Evaluating alternative best hyperparameters...")

alt_best_params = {
    'subsample': float(1.0),
    'reg_lambda': float(0.5),
    'reg_alpha': float(0.5),
    'n_estimators': 441,
    'max_depth': 3,
    'learning_rate': float(0.11105047448957144),
    'gamma': float(4.512764533397833),
    'colsample_bytree': float(0.641249547534373)
}

for target in target_list:
    print(f"\n Evaluating target: {target}")

    X_train = X_train_targets[target]
    y_train = y_train_targets[target]
    X_val = X_val_targets[target]
    y_val = y_val_targets[target]

    # Initialize and fit model with alternative params
    model = XGBRegressor(**alt_best_params, random_state=42, verbosity=0)
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_val)

    # Evaluate RMSE and Directional Accuracy
    rmse = np.sqrt(mean_squared_error(y_val, y_pred))
    direction_true = np.sign(y_val)
    direction_pred = np.sign(y_pred)
    directional_accuracy = accuracy_score(direction_true, direction_pred)

    # Store results
    alt_eval_results[target] = {
        'RMSE': rmse,
        'Directional Accuracy': directional_accuracy
    }

    print(f" RMSE: {rmse:.5f}")
    print(f" Directional Accuracy: {directional_accuracy:.2%}")

"""Final Hyperparameter tuning


"""

from sklearn.model_selection import RandomizedSearchCV, GroupKFold
from xgboost import XGBRegressor
from scipy.stats import uniform, randint
import numpy as np

#  Define wider search space
param_dist = {
    'n_estimators': randint(100, 900),
    'learning_rate': uniform(0.01, 0.29),
    'max_depth': randint(3, 10),
    'gamma': uniform(0, 5),
    'colsample_bytree': uniform(0.5, 0.5),
    'subsample': uniform(0.5, 0.5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}

#  GroupKFold cross-validator to avoid leakage
cv = GroupKFold(n_splits=5)

#  Base XGB Regressor
xgb_base = XGBRegressor(random_state=42, verbosity=0)

#  Robust Randomized Search
random_search = RandomizedSearchCV(
    estimator=xgb_base,
    param_distributions=param_dist,
    n_iter=200,  # ğŸ” Increase number of random trials
    scoring='neg_root_mean_squared_error',
    cv=cv,
    verbose=2,
    n_jobs=-1,
    random_state=42
)

#  Run the search
random_search.fit(X_train, y_train, groups=groups)

#  Best parameters and score
print("Best Parameters:", random_search.best_params_)
print("Best RMSE:", -random_search.best_score_)

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

#  Best parameters from the latest RandomizedSearchCV
robust_best_params = {
    'colsample_bytree': float(0.5694123867210101),
    'gamma': float(0.013554497071648353),
    'learning_rate': float(0.04384185860108758),
    'max_depth': 9,
    'n_estimators': 649,
    'reg_alpha': float(0.41184091414726853),
    'reg_lambda': float(0.3488682665429953),
    'subsample': float(0.9647645721239129)
}

print("ğŸ” Evaluating robust hyperparameters...\n")

for target in target_list:
    print(f"ğŸ“Œ Evaluating target: {target}")
    X_train = X_train_targets[target]
    y_train = y_train_targets[target]
    X_val = X_val_targets[target]
    y_val = y_val_targets[target]

    # Train model
    model = XGBRegressor(**robust_best_params, random_state=42, verbosity=0)
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_val)

    #  RMSE (manual computation)
    mse = mean_squared_error(y_val, y_pred)
    rmse = np.sqrt(mse)
    print(f" RMSE: {rmse:.5f}")

    #  Directional Accuracy
    direction_true = np.sign(y_val)
    direction_pred = np.sign(y_pred)
    directional_accuracy = (direction_true == direction_pred).mean() * 100
    print(f" Directional Accuracy: {directional_accuracy:.2f}%\n")

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error

# Best parameter sets
manual_best_params = {
    'colsample_bytree': 0.8,
    'gamma': 0.0,
    'learning_rate': 0.08,
    'max_depth': 5,
    'n_estimators': 400,
    'reg_alpha': 0.5,
    'reg_lambda': 0.5,
    'subsample': 1.0
}

robust_best_params = {
    'colsample_bytree': 0.5694123867210101,
    'gamma': 0.013554497071648353,
    'learning_rate': 0.04384185860108758,
    'max_depth': 9,
    'n_estimators': 649,
    'reg_alpha': 0.41184091414726853,
    'reg_lambda': 0.3488682665429953,
    'subsample': 0.9647645721239129
}

# Select model for each target
model_choices = {
    'target_return_1d': manual_best_params,
    'target_return_5d': robust_best_params,
    'target_return_20d': robust_best_params
}

print(" Evaluating final ensemble of best models...\n")

for target in target_list:
    print(f" Evaluating target: {target}")

    X_train = X_train_targets[target]
    y_train = y_train_targets[target]
    X_val = X_val_targets[target]
    y_val = y_val_targets[target]

    # Pick correct hyperparameters
    best_params = model_choices[target]

    # Train model
    model = XGBRegressor(**best_params, random_state=42, verbosity=0)
    model.fit(X_train, y_train)

    # Predict
    y_pred = model.predict(X_val)

    # Evaluate RMSE (manual due to squared=False bug)
    rmse = (mean_squared_error(y_val, y_pred))**0.5
    print(f" RMSE: {rmse:.5f}")

    # Directional accuracy
    direction = (y_pred > 0) == (y_val > 0)
    direction_acc = direction.mean() * 100
    print(f" Directional Accuracy: {direction_acc:.2f}%\n")

from sklearn.inspection import permutation_importance
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor
import pandas as pd
import numpy as np

feature_importance_results = {}

print("ğŸ” Computing permutation importances on test set...\n")

for target in target_list:
    print(f" Target: {target}")

    # Get features used in previous runs
    X_cols = X_val_targets[target].columns.tolist()
    X_cols = [col for col in X_cols if col in trainval_df.columns]

    # Clean trainval and test sets
    trainval_clean = trainval_df[trainval_df[target].notna()].dropna(subset=X_cols)
    test_clean = test_df[test_df[target].notna()].dropna(subset=X_cols)

    X_train = trainval_clean[X_cols].astype('float32')
    y_train = trainval_clean[target]
    X_test = test_clean[X_cols].astype('float32')
    y_test = test_clean[target]

    # Fit model with best parameters
    model = XGBRegressor(**model_choices[target], random_state=42, verbosity=0)
    model.fit(X_train, y_train)

    # Compute permutation importance on test data
    result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42,
                                    scoring='neg_root_mean_squared_error')

    importance_df = pd.DataFrame({
        'feature': X_test.columns,
        'importance_mean': result.importances_mean,
        'importance_std': result.importances_std
    }).sort_values(by='importance_mean', ascending=False)

    feature_importance_results[target] = importance_df
    print(importance_df.head(10), "\n")

import matplotlib.pyplot as plt
import seaborn as sns

def compare_distributions(val_df, test_df, features, target, bins=50):
    print(f"\n Distribution check for: {target}")

    # --- Target Distribution ---
    print(" Target stats:")
    print(f"  Val Mean:  {val_df[target].mean():.5f}, Std: {val_df[target].std():.5f}")
    print(f"  Test Mean: {test_df[target].mean():.5f}, Std: {test_df[target].std():.5f}")

    plt.figure(figsize=(12, 4))
    sns.histplot(val_df[target], bins=bins, color='blue', label='Validation', kde=True, stat="density")
    sns.histplot(test_df[target], bins=bins, color='red', label='Test', kde=True, stat="density")
    plt.title(f"Target Distribution: {target}")
    plt.legend()
    plt.show()

    # --- Feature Distributions ---
    for feat in features:
        if feat not in val_df.columns or feat not in test_df.columns:
            continue

        val_feat = val_df[feat].dropna()
        test_feat = test_df[feat].dropna()

        if val_feat.std() == 0 and test_feat.std() == 0:
            continue  # skip constant features

        plt.figure(figsize=(10, 3))
        sns.histplot(val_feat, bins=bins, color='blue', label='Validation', kde=True, stat="density")
        sns.histplot(test_feat, bins=bins, color='red', label='Test', kde=True, stat="density")
        plt.title(f"Feature: {feat}")
        plt.legend()
        plt.show()

# Compare distributions for each target
for target in target_list:
    X_cols = X_val_targets[target].columns.tolist()
    X_cols = [col for col in X_cols if col in val_df.columns and col in test_df.columns]

    val_clean = val_df[val_df[target].notna()].dropna(subset=X_cols)
    test_clean = test_df[test_df[target].notna()].dropna(subset=X_cols)

    compare_distributions(val_clean, test_clean, features=X_cols, target=target)

"""##Final Model"""

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd  # Ensure pandas is imported

target_list = ['target_return_1d', 'target_return_5d', 'target_return_20d']

manual_best_params = {
    'colsample_bytree': 0.8,
    'gamma': 0.0,
    'learning_rate': 0.08,
    'max_depth': 5,
    'n_estimators': 400,
    'reg_alpha': 0.5,
    'reg_lambda': 0.5,
    'subsample': 1.0
}

robust_best_params = {
    'colsample_bytree': 0.5694123867210101,
    'gamma': 0.013554497071648353,
    'learning_rate': 0.04384185860108758,
    'max_depth': 9,
    'n_estimators': 649,
    'reg_alpha': 0.41184091414726853,
    'reg_lambda': 0.3488682665429953,
    'subsample': 0.9647645721239129
}

model_choices = {
    'target_return_1d': manual_best_params,
    'target_return_5d': robust_best_params,
    'target_return_20d': robust_best_params
}

#  FIX: Convert period[M] -> integer month in ALL splits
for df in [train_df, val_df, test_df]:
    if 'month' in df.columns and str(df['month'].dtype).startswith("period"):
        df['month'] = df['month'].dt.month.astype(int)

trainval_df = pd.concat([train_df, val_df], axis=0)

print("ğŸ” Final evaluation on test set using Train+Val for training...\n")

for target in target_list:
    print(f"ğŸ“Œ Training & Evaluating target: {target}")

    # Use feature list from previous successful run
    X_cols = X_val_targets[target].columns.tolist()

    # Drop rows with missing target or feature values
    trainval_clean = trainval_df[trainval_df[target].notna()].dropna(subset=X_cols)
    test_clean = test_df[test_df[target].notna()].dropna(subset=X_cols)

    #  Reconfirm that all features are valid numeric types
    valid_cols = []
    for col in X_cols:
        dtype = trainval_clean[col].dtype
        if np.issubdtype(dtype, np.number) or dtype.name in ['category', 'bool', 'int64', 'float64']:
            valid_cols.append(col)
    X_cols = valid_cols

    # Final input-output matrices
    X_train = trainval_clean[X_cols]
    y_train = trainval_clean[target]
    X_test = test_clean[X_cols]
    y_test = test_clean[target]

    print(f"Train+Val Shape: {X_train.shape} | Test Shape: {X_test.shape}")

    best_params = model_choices[target]
    model = XGBRegressor(**best_params, random_state=42, verbosity=0)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred) ** 0.5
    direction_acc = ((y_pred > 0) == (y_test > 0)).mean() * 100

    print(f" RMSE: {rmse:.5f}")
    print(f" Directional Accuracy: {direction_acc:.2f}%\n")

trainval_df = pd.concat([train_df, val_df], axis=0)

# Convert period month to integer if necessary
if 'month' in trainval_df.columns and str(trainval_df['month'].dtype).startswith("period"):
    trainval_df['month'] = trainval_df['month'].dt.month.astype(int)

from sklearn.model_selection import RandomizedSearchCV, GroupKFold
from xgboost import XGBRegressor
from scipy.stats import uniform, randint
import numpy as np
import pandas as pd

#  Define hyperparameter search space
param_dist = {
    'n_estimators': randint(100, 900),
    'learning_rate': uniform(0.01, 0.29),
    'max_depth': randint(3, 10),
    'gamma': uniform(0, 5),
    'colsample_bytree': uniform(0.5, 0.5),
    'subsample': uniform(0.5, 0.5),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1)
}

#  GroupKFold for time-series-style validation
cv = GroupKFold(n_splits=5)

#  Store best params per target
new_best_params = {}

#  Ensure 'month' is numeric in trainval_df if it exists
if 'month' in trainval_df.columns and str(trainval_df['month'].dtype).startswith("period"):
    trainval_df['month'] = trainval_df['month'].dt.month.astype(int)

#  Tune for each target
for target in target_list:
    print(f" Tuning hyperparameters for {target}...")

    # Get feature list used in previous successful validation runs
    X_cols = X_val_targets[target].columns.tolist()

    #  Ensure 'month' is numeric if present in this specific run
    if 'month' in X_cols and 'month' in trainval_df.columns:
        if str(trainval_df['month'].dtype).startswith("period"):
            trainval_df['month'] = trainval_df['month'].dt.month.astype(int)

    #  Drop columns not found in trainval_df (fix KeyError issue)
    existing_cols = [col for col in X_cols if col in trainval_df.columns]
    missing_cols = list(set(X_cols) - set(existing_cols))
    if missing_cols:
        print(f" Dropping missing columns from feature list: {missing_cols}")
    X_cols = existing_cols

    #  Drop missing rows from both features and target
    df_clean = trainval_df[trainval_df[target].notna()].dropna(subset=X_cols)

    # Final input matrices (ensure float32)
    X = df_clean[X_cols].astype('float32')
    y = df_clean[target]

    #  Grouping variable to avoid leakage by entity (if available)
    groups = df_clean['entity_id'] if 'entity_id' in df_clean.columns else np.arange(len(df_clean))

    #  Randomized Search CV
    search = RandomizedSearchCV(
        estimator=XGBRegressor(random_state=42, verbosity=0),
        param_distributions=param_dist,
        n_iter=200,
        scoring='neg_root_mean_squared_error',
        cv=cv,
        verbose=2,
        n_jobs=-1,
        random_state=42
    )

    #  Run tuning
    search.fit(X, y, groups=groups)

    #  Save best parameters
    new_best_params[target] = search.best_params_
    print(f"âœ… Best params for {target}: {search.best_params_}")
    print(f"âœ… Best RMSE: {-search.best_score_:.5f}\n")

from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

print(" Final evaluation on test set using Train+Val for training...\n")

for target in target_list:
    print(f" Training & Evaluating target: {target}")

    X_cols = X_val_targets[target].columns.tolist()

    # Ensure 'month' isn't included if it's not in trainval_df
    X_cols = [col for col in X_cols if col in trainval_df.columns]

    # Clean input data
    trainval_clean = trainval_df[trainval_df[target].notna()].dropna(subset=X_cols)
    test_clean = test_df[test_df[target].notna()].dropna(subset=X_cols)

    X_train = trainval_clean[X_cols].astype('float32')
    y_train = trainval_clean[target]
    X_test = test_clean[X_cols].astype('float32')
    y_test = test_clean[target]

    print(f"Train+Val Shape: {X_train.shape} | Test Shape: {X_test.shape}")

    model = XGBRegressor(**model_choices[target], random_state=42, verbosity=0)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred) ** 0.5
    direction_acc = ((y_pred > 0) == (y_test > 0)).mean() * 100

    print(f" RMSE: {rmse:.5f}")
    print(f" Directional Accuracy: {direction_acc:.2f}%\n")

"""#Visualization of Results

#Old Code

---

##Feature Selection
"""

# Define columns to exclude
exclude_cols = [
    'entity_name', 'ticker', 'date', 'shock_polarity',
    'is_volatility_shock', 'is_vol_shock_day', 'prev_date', 'day_diff',
    'new_streak', 'streak_id', 'entity_ID',  # entity_ID_encoded is kept
    'target_return_1d', 'target_return_5d', 'target_return_20d'
]

# Candidate feature list
candidate_features = [col for col in train_df.columns if col not in exclude_cols]
print("Number of candidate features:", len(candidate_features))
print(candidate_features)

"""Feature Correllation and Redundancy Check"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Select only numeric features from train_df
numeric_features = train_df.select_dtypes(include=[np.number])

# Compute correlation matrix
correlation_matrix = numeric_features.corr()

# Plot heatmap of correlation matrix
plt.figure(figsize=(18, 14))
sns.heatmap(correlation_matrix, cmap='coolwarm', center=0, annot=False)
plt.title('Feature Correlation Heatmap')
plt.show()

# Find pairs of features with high correlation (absolute value > 0.85)
threshold = 0.85
high_corr_pairs = []

for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        if abs(corr_value) > threshold:
            high_corr_pairs.append((correlation_matrix.columns[i],
                                    correlation_matrix.columns[j],
                                    corr_value))

# Sort pairs by absolute correlation
high_corr_pairs = sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)

# Display highly correlated feature pairs
high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Feature 1', 'Feature 2', 'Correlation'])
print("\nHighly correlated feature pairs (|correlation| > 0.85):")
display(high_corr_df)

"""Dropping redundant and high collinear features"""

# Drop the selected redundant features from all datasets
drop_features = [
    'ema_10', 'bb_mid', 'bb_upper', 'bb_lower',
    'total_equity_2024', 'interest_expense_2024', 'total_debt_2024', 'total_assets_2024',
    'quarter', 'week_of_year',
    'book_to_market_missing'
]

for df in [train_df, val_df, test_df]:
    df.drop(columns=drop_features, inplace=True, errors='ignore')

print("Features dropped. Remaining columns:")
print(train_df.columns.tolist())

"""Permutation Importance -- Feature Selection

Permutation Importance for target 1d

we have excluded 'open' since open and close are so highly correllated
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Define features (exclude 'open')
features_to_use = [col for col in train_df.columns if col not in [
    'open', 'date', 'ticker', 'entity_name', 'entity_ID',
    'prev_date', 'shock_polarity',  # also non-numeric or not useful
    'target_return_1d', 'target_return_5d', 'target_return_20d'
]]

# Target
target_col = 'target_return_1d'

# Prepare data
X_train = train_df[features_to_use]
y_train = train_df[target_col]
X_val = val_df[features_to_use]
y_val = val_df[target_col]

# Train baseline model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Permutation importance on validation set
perm_importance = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42, scoring='neg_mean_squared_error')

# Format results
perm_df = pd.DataFrame({
    'feature': X_val.columns,
    'importance_mean': perm_importance.importances_mean,
    'importance_std': perm_importance.importances_std
}).sort_values(by='importance_mean', ascending=False)

print("\nğŸ”¹ Top Features for target_return_1d:")
display(perm_df.head(20))

"""Permutation Importance for target return 20d"""

# Change target
target_col = 'target_return_20d'

# Prepare target
y_train = train_df[target_col]
y_val = val_df[target_col]

# Refit model
model.fit(X_train, y_train)

# Permutation importance
perm_importance_20d = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42, scoring='neg_mean_squared_error')

# Format results
perm_df_20d = pd.DataFrame({
    'feature': X_val.columns,
    'importance_mean': perm_importance_20d.importances_mean,
    'importance_std': perm_importance_20d.importances_std
}).sort_values(by='importance_mean', ascending=False)

print("\nğŸ”¹ Top Features for target_return_20d:")
display(perm_df_20d.head(20))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib_venn import venn3

# Manually define feature sets from your earlier outputs
top_1d = {
    'day_of_month', 'new_streak', 'avg_market_volatility', 'market_cap', 'streak_id',
    'was_not_rated', 'pbv_x_2024_missing', 'credit_rating_rank_missing',
    'book_value_equity_missing', 'roe_missing', 'debt_to_equity_2024_missing',
    'is_volatility_shock', 'daily_return', 'total_rev_2024_missing',
    'net_income_missing', 'roe', 'book_value_equity', 'net_income',
    'credit_rating_rank', 'book_to_market'
}

top_5d = {
    'rsi_14', 'daily_return', 'avg_return', 'day_of_month', 'intraday_volatility',
    'return_z_score', 'return_std', 'market_cap', 'momentum_10d', 'net_income',
    'avg_volume_20d', 'streak_id', 'entity_ID_encoded', 'pbv_x_2024_missing',
    'net_income_missing', 'book_value_equity_missing', 'debt_to_equity_2024_missing',
    'roe_missing', 'is_volatility_shock', 'credit_rating_rank_missing'
}

top_20d = {
    'day_of_week', 'avg_return', 'rsi_14', 'momentum_10d', 'avg_volume_20d',
    'new_streak', 'return_z_score', 'avg_market_volatility', 'return_std',
    'credit_rating_rank_missing', 'volume', 'intraday_volatility',
    'total_rev_2024_missing', 'pbv_x_2024_missing', 'net_income_missing',
    'book_value_equity_missing', 'debt_to_equity_2024_missing', 'roe_missing',
    'is_volatility_shock', 'was_not_rated'
}

# Full set of known features
all_features = sorted(list(
    top_1d.union(top_5d).union(top_20d).union({
        'open', 'close', 'total_assets_2024', 'total_debt_2024', 'total_equity_2024',
        'common_dividend_declared_2024', 'pbv_x_2024', 'debt_to_equity_2024',
        'sma_10', 'ema_10', 'bb_mid', 'bb_upper', 'bb_lower', 'month',
        'quarter', 'week_of_year', 'entity_ID', 'entity_name', 'ticker', 'date'
    })
))

# Build summary matrix
presence_data = []
for feat in all_features:
    row = {
        'Feature': feat,
        'In_1D': int(feat in top_1d),
        'In_5D': int(feat in top_5d),
        'In_20D': int(feat in top_20d)
    }
    total = row['In_1D'] + row['In_5D'] + row['In_20D']
    if total == 3:
        row['Horizon_Summary'] = 'All 3'
    elif total == 2:
        row['Horizon_Summary'] = 'Only 2'
    elif total == 1:
        row['Horizon_Summary'] = 'Only 1'
    else:
        row['Horizon_Summary'] = 'None'
    presence_data.append(row)

# Convert to DataFrame
presence_df = pd.DataFrame(presence_data)

# Display master summary table
print("ğŸ“Š Master Feature-Horizon Table")
display(presence_df.sort_values(by="Horizon_Summary", ascending=True))

# Heatmap
plt.figure(figsize=(12, len(presence_df) * 0.25))
sns.heatmap(
    presence_df[['In_1D', 'In_5D', 'In_20D']].set_index(presence_df['Feature']),
    cmap='YlGnBu', cbar=False, linewidths=0.5, linecolor='gray'
)
plt.title("Feature Presence in Top 20 (Permutation Importance)", fontsize=14)
plt.xlabel("Target Horizon")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

# Venn diagram
plt.figure(figsize=(6, 6))
venn3([top_1d, top_5d, top_20d],
      set_labels=('target_return_1d', 'target_return_5d', 'target_return_20d'))
plt.title("Feature Overlap Across Horizons")
plt.show()

"""###Model Specific Feature Selection Steps

Train a rudimentarty NN model -  feedforward neural network (FNN) to run permutation importance on a trained NN model. This way, we will get NN specific feature importance scoring
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout
from tensorflow.keras.models import Model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# === Step 1: Prepare Inputs ===

# Define target horizon
target_col = 'target_return_1d'  # or 'target_return_5d' / 'target_return_20d'

# Drop target and entity_ID_encoded from numeric inputs
drop_cols = ['target_return_1d', 'target_return_5d', 'target_return_20d', 'entity_ID_encoded']
numeric_cols = [col for col in train_df.select_dtypes(include=[np.number]).columns if col not in drop_cols]

# Extract numeric features and cast to float32
X_train_num = train_df[numeric_cols].astype(np.float32)
X_val_num = val_df[numeric_cols].astype(np.float32)
X_test_num = test_df[numeric_cols].astype(np.float32)

# Extract entity ID as separate input (int32 for embedding)
X_train_entity = train_df['entity_ID_encoded'].astype(np.int32).values
X_val_entity = val_df['entity_ID_encoded'].astype(np.int32).values
X_test_entity = test_df['entity_ID_encoded'].astype(np.int32).values

# Extract targets (you can change target_col above)
y_train = train_df[target_col].astype(np.float32).values
y_val = val_df[target_col].astype(np.float32).values
y_test = test_df[target_col].astype(np.float32).values

# === Step 2: Build Model ===

# Input layers
numeric_input = Input(shape=(X_train_num.shape[1],), name='numeric_input')
entity_input = Input(shape=(1,), dtype='int32', name='entity_input')

# Embedding layer
n_entities = int(max(X_train_entity.max(), X_val_entity.max(), X_test_entity.max()) + 1)
embed_dim = 8
entity_embedding = Embedding(input_dim=n_entities, output_dim=embed_dim, name='entity_embedding')(entity_input)
entity_embedding = Flatten()(entity_embedding)

# Combine embeddings and numeric features
x = Concatenate()([numeric_input, entity_embedding])

# Dense layers
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(64, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(1, activation='linear')(x)

# Model
model = Model(inputs={'numeric_input': numeric_input, 'entity_input': entity_input}, outputs=output)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# === Step 3: Train Model ===
history = model.fit(
    {'numeric_input': X_train_num, 'entity_input': X_train_entity},
    y_train,
    validation_data=(
        {'numeric_input': X_val_num, 'entity_input': X_val_entity},
        y_val
    ),
    epochs=10,
    batch_size=64,
    verbose=1
)

# === Step 4: Evaluate ===
y_pred = model.predict({'numeric_input': X_val_num, 'entity_input': X_val_entity}).flatten()

mae = mean_absolute_error(y_val, y_pred)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
r2 = r2_score(y_val, y_pred)

print(f"\nValidation Results for {target_col}:")
print(f"MAE:  {mae:.6f}")
print(f"RMSE: {rmse:.6f}")
print(f"RÂ²:   {r2:.6f}")

"""Now for 5 day and 20 day target horizons"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, Concatenate, Flatten
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

def build_and_train_nn_model(X_train_num, X_val_num, X_test_num,
                             X_train_entity, X_val_entity, X_test_entity,
                             y_train, y_val, target_name,
                             embedding_input_dim=5000, embedding_output_dim=8,
                             epochs=10, batch_size=32):
    import tensorflow as tf
    from tensorflow.keras import layers, models

    # === Step 1: Ensure correct dtypes ===
    X_train_num = X_train_num.astype(np.float32)
    X_val_num = X_val_num.astype(np.float32)
    X_test_num = X_test_num.astype(np.float32)

    # === Step 2: Define Inputs ===
    input_numeric = layers.Input(shape=(X_train_num.shape[1],), name='numeric_input')
    input_entity = layers.Input(shape=(1,), dtype='int32', name='entity_input')

    # === Step 3: Entity Embedding Layer ===
    x_entity = layers.Embedding(input_dim=embedding_input_dim, output_dim=embedding_output_dim)(input_entity)
    x_entity = layers.Flatten()(x_entity)

    # === Step 4: Concatenate numeric + embedding ===
    x = layers.Concatenate()([input_numeric, x_entity])
    x = layers.Dense(64, activation='relu')(x)
    x = layers.Dense(32, activation='relu')(x)
    output = layers.Dense(1, activation='linear')(x)

    model = models.Model(inputs=[input_numeric, input_entity], outputs=output)

    # âœ… FIXED: Use correct loss
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])

    # === Step 5: Train the model ===
    history = model.fit(
        {'numeric_input': X_train_num, 'entity_input': X_train_entity},
        y_train,
        validation_data=({'numeric_input': X_val_num, 'entity_input': X_val_entity}, y_val),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1
    )

    # === Step 6: Evaluate on validation set ===
    val_preds = model.predict({'numeric_input': X_val_num, 'entity_input': X_val_entity})
    mae = mean_absolute_error(y_val, val_preds)
    rmse = np.sqrt(mean_squared_error(y_val, val_preds))
    r2 = r2_score(y_val, val_preds)

    print(f"\nValidation Results for {target_name}:")
    print(f"MAE:  {mae:.6f}")
    print(f"RMSE: {rmse:.6f}")
    print(f"RÂ²:   {r2:.6f}")

    return model

"""training above 5/20day horizons models"""

# Define target for 1-day return again
y_train_1d = train_df['target_return_1d'].values
y_val_1d = val_df['target_return_1d'].values

# Retrain the 1-day model
model_1d = build_and_train_nn_model(
    X_train_num, X_val_num, X_test_num,
    X_train_entity, X_val_entity, X_test_entity,
    y_train_1d, y_val_1d, 'target_return_1d'
)

# Define target arrays for 5-day and 20-day returns
y_train_5d = train_df['target_return_5d'].values
y_val_5d = val_df['target_return_5d'].values

y_train_20d = train_df['target_return_20d'].values
y_val_20d = val_df['target_return_20d'].values

# Train for 5-day return
model_5d = build_and_train_nn_model(
    X_train_num, X_val_num, X_test_num,
    X_train_entity, X_val_entity, X_test_entity,
    y_train_5d, y_val_5d, 'target_return_5d'
)

# Train for 20-day return
model_20d = build_and_train_nn_model(
    X_train_num, X_val_num, X_test_num,
    X_train_entity, X_val_entity, X_test_entity,
    y_train_20d, y_val_20d, 'target_return_20d'
)

"""Permutation importance for all 3 time horizons"""

from sklearn.inspection import permutation_importance
from sklearn.base import BaseEstimator, RegressorMixin
import numpy as np
import pandas as pd

# Step 1: Define wrapper
class KerasRegressorWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, model, entity_input):
        self.model = model
        self.entity_input = entity_input

    def fit(self, X, y):
        # Not used â€” model already trained
        return self

    def predict(self, X):
        return self.model.predict({'numeric_input': X, 'entity_input': self.entity_input}, verbose=0).flatten()

# Step 2: Function to run permutation
def compute_permutation_importance(model, X_val_num, X_val_entity, y_val, feature_names, horizon_label):
    wrapper = KerasRegressorWrapper(model, X_val_entity)
    result = permutation_importance(wrapper, X_val_num, y_val, n_repeats=10, random_state=42, scoring='neg_mean_absolute_error')

    importances = pd.DataFrame({
        'feature': feature_names,
        f'importance_mean_{horizon_label}': result.importances_mean,
        f'importance_std_{horizon_label}': result.importances_std
    }).sort_values(by=f'importance_mean_{horizon_label}', ascending=False)

    return importances.reset_index(drop=True)

# Step 3: Prepare feature names
feature_names = X_val_num.columns.tolist()

# Step 4: Run permutation for each model horizon
perm_1d = compute_permutation_importance(model_1d, X_val_num, X_val_entity, y_val_1d, feature_names, '1d')
perm_5d = compute_permutation_importance(model_5d, X_val_num, X_val_entity, y_val_5d, feature_names, '5d')
perm_20d = compute_permutation_importance(model_20d, X_val_num, X_val_entity, y_val_20d, feature_names, '20d')

# Merge all 3 results
merged_importance = perm_1d.merge(perm_5d, on='feature').merge(perm_20d, on='feature')

# Round for easier reading
merged_importance = merged_importance.round(6)

# Sort by average importance across all 3 horizons
merged_importance['avg_importance'] = merged_importance[[col for col in merged_importance.columns if 'importance_mean' in col]].mean(axis=1)
merged_importance = merged_importance.sort_values(by='avg_importance', ascending=False)

# Display top 25
merged_importance.head(25)

# Rank features by importance (lower rank = higher importance)
ranked = merged_importance.copy()
ranked['rank_1d'] = ranked['importance_mean_1d'].rank(ascending=False)
ranked['rank_5d'] = ranked['importance_mean_5d'].rank(ascending=False)
ranked['rank_20d'] = ranked['importance_mean_20d'].rank(ascending=False)

# Average rank and sort
ranked['avg_rank'] = ranked[['rank_1d', 'rank_5d', 'rank_20d']].mean(axis=1)
least_important = ranked.sort_values(by='avg_rank', ascending=True).tail(15)
least_important[['feature', 'avg_rank', 'rank_1d', 'rank_5d', 'rank_20d']]

import seaborn as sns
import matplotlib.pyplot as plt

# Heatmap of top 20 features across 3 horizons
top_features = merged_importance.head(20).set_index('feature')
sns.heatmap(top_features[[c for c in top_features.columns if 'importance_mean' in c]],
            cmap='YlGnBu', annot=True, fmt=".5f")
plt.title('Top 20 Feature Importances Across Prediction Horizons')
plt.show()

"""Features Removed:

Feature	Reason
avg_return	Explicitly marked for removal (very low importance)
intraday_volatility	Explicitly marked for removal
pbv_x_2024	Very low importance
pbv_x_2024_missing	Minimal contribution
day_of_week	Very low ranks (30+ across horizons)
day_of_month	Very low ranks (30+ across horizons)
return_std	Redundant with return_z_score, low rank
roe, roe_missing	Minimal impact
debt_to_equity_2024_missing, book_value_equity_missing	Weak contribution
"""

final_features = [
    'daily_return',
    'open',
    'return_z_score',
    'avg_volume_20d',
    'market_cap',
    'net_income',
    'credit_rating_rank',
    'book_to_market',
    'book_value_equity',
    'close',
    'volume',
    'was_not_rated',
    'common_dividend_declared_2024',
    'sma_10',
    'total_rev_2024',
    'credit_rating_rank_missing',
    'streak_id',
    'avg_market_volatility',
    'month',
    'debt_to_equity_2024',
    'rsi_14',
    'momentum_10d'
]

# # Step 1: Get the list of (entity_ID, date) for each shock
# shock_pairs = vol_df_global[['entity_ID', 'date']].drop_duplicates()

# # Step 2: Check for 60-day windows in master_df
# def check_window_from_master(row, days=60):
#     entity = row['entity_ID']
#     event_date = row['date']
#     entity_data = master_df[master_df['entity_ID'] == entity]

#     before = entity_data[entity_data['date'] < event_date].tail(days)
#     after = entity_data[entity_data['date'] > event_date].head(days)

#     return pd.Series({
#         'has_60_before': len(before) == days,
#         'has_60_after': len(after) == days
#     })

# # Step 3: Run checks
# window_check_master = shock_pairs.apply(check_window_from_master, axis=1)

# # Step 4: View summary
# print(window_check_master.value_counts())

"""---

##Create embedding layer for entity_ID
"""

from sklearn.preprocessing import LabelEncoder

# Combine all entity_IDs across splits to ensure consistent encoding
all_entity_ids = pd.concat([train_df['entity_ID'], val_df['entity_ID'], test_df['entity_ID']])

# Initialize and fit encoder
le = LabelEncoder()
le.fit(all_entity_ids)

# Store the mapping (useful for the embedding layer's input_dim)
entity_id_to_index = {entity_id: idx for idx, entity_id in enumerate(le.classes_)}
num_unique_entities = len(entity_id_to_index)

# Apply encoding to each dataset
train_df['entity_ID_encoded'] = le.transform(train_df['entity_ID'])
val_df['entity_ID_encoded'] = le.transform(val_df['entity_ID'])
test_df['entity_ID_encoded'] = le.transform(test_df['entity_ID'])

print(f"Number of unique entities: {num_unique_entities}")
print(train_df[['entity_ID', 'entity_ID_encoded']].drop_duplicates().head())

train_df.head(5)

"""# NN Model"""

# Step 1: Generate delta price targets (price(t+h) - price(t)) on unscaled master_df
target_horizons = [1, 5, 20]
for horizon in target_horizons:
    master_df[f'target_delta_{horizon}d'] = (
        master_df.groupby('entity_ID')['close'].shift(-horizon) - master_df['close']
    )

# Step 2: Create lightweight DataFrame with only targets and merge keys
target_cols = ['entity_ID', 'date'] + [f'target_delta_{h}d' for h in target_horizons]
targets_df = master_df[target_cols].copy()

# Step 3: Merge into final_shocks (this overwrites old target columns, so rerunning is safe)
print("âœ… Columns in final_shocks BEFORE merge:", final_shocks.columns.tolist())
print("âœ… Columns in targets_df:", targets_df.columns.tolist())

final_shocks = final_shocks.merge(targets_df, on=['entity_ID', 'date'], how='left')

# Step 4: Validate target columns were added correctly
missing_cols = [col for col in [f'target_delta_{h}d' for h in target_horizons] if col not in final_shocks.columns]
if missing_cols:
    raise ValueError(f"âŒ Missing target columns after merge: {missing_cols}")

# Step 5: Drop shocks with any missing target values
shock_targets = final_shocks.dropna(subset=[f'target_delta_{h}d' for h in target_horizons]).reset_index(drop=True)

# Final confirmation
print("âœ… Delta price targets successfully added to final_shocks.")
print(f"âœ… Remaining valid shocks after dropping missing delta targets: {len(shock_targets)}")

"""NN Model"""

from sklearn.preprocessing import StandardScaler

# Extract raw targets
y_cols = ['target_delta_1d', 'target_delta_5d', 'target_delta_20d']
y_lstm_raw = shock_targets[y_cols].iloc[:len(X_lstm)].values

# Save unscaled version
y_lstm_unscaled = y_lstm_raw.copy()

# Fit scaler on training portion only
scaler = StandardScaler()
scaler.fit(y_lstm_raw[sequence_df['date'] <= "2024-06-30"])

# Apply scaler
y_lstm_scaled = scaler.transform(y_lstm_raw)

# Build entity_ID array
entity_array = shock_targets['entity_ID'].iloc[:len(X_lstm)].values

# Add full sequence-level dataframe
sequence_df = pd.DataFrame({
    'entity_ID': entity_array,
    'date': pd.to_datetime(shock_targets['date'].iloc[:len(X_lstm)].values)
})

# Split by time
train_idx = sequence_df[sequence_df['date'] <= "2024-06-30"].index
val_idx   = sequence_df[(sequence_df['date'] > "2024-06-30") & (sequence_df['date'] <= "2024-08-31")].index

# Raw splits
X_train, y_train, entity_train = X_lstm[train_idx], y_lstm_scaled[train_idx], entity_array[train_idx]
X_val,   y_val,   entity_val   = X_lstm[val_idx],   y_lstm_scaled[val_idx],   entity_array[val_idx]

# Remove NaN sequences
nan_train = np.any(np.isnan(X_train), axis=(1, 2)) | np.any(np.isnan(y_train), axis=1)
nan_val = np.any(np.isnan(X_val), axis=(1, 2)) | np.any(np.isnan(y_val), axis=1)

X_train_clean = X_train[~nan_train]
y_train_clean = y_train[~nan_train]
entity_train_clean = entity_train[~nan_train]

X_val_clean = X_val[~nan_val]
y_val_clean = y_val[~nan_val]
entity_val_clean = entity_val[~nan_val]

import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, Concatenate, RepeatVector, Lambda
from tensorflow.keras.models import Model

# Define custom attention layer
class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),
                                 initializer='random_normal', trainable=True)
        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),
                                 initializer='zeros', trainable=True)
        super(AttentionLayer, self).build(input_shape)

    def call(self, inputs):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(inputs, self.W) + self.b)
        alpha = tf.keras.backend.softmax(e, axis=1)
        context = inputs * alpha
        return tf.keras.backend.sum(context, axis=1)

# Entity encoder
from sklearn.preprocessing import LabelEncoder
entity_encoder = LabelEncoder()
entity_encoder.fit(shock_targets['entity_ID'])
entity_train_encoded = entity_encoder.transform(entity_train_clean)
entity_val_encoded = entity_encoder.transform(entity_val_clean)

# Define full model
def build_entity_attention_lstm(input_seq_shape, n_entities, emb_dim=8, lstm_units=64, dropout_rate=0.2):
    sequence_input = Input(shape=input_seq_shape, name='sequence_input')
    entity_input = Input(shape=(1,), name='entity_input')

    x = Embedding(input_dim=n_entities, output_dim=emb_dim)(entity_input)
    x = Lambda(lambda z: tf.squeeze(z, axis=1))(x)
    x = RepeatVector(input_seq_shape[0])(x)

    merged = Concatenate(axis=-1)([sequence_input, x])

    x = LSTM(lstm_units, return_sequences=True)(merged)
    x = Dropout(dropout_rate)(x)
    x = LSTM(lstm_units, return_sequences=True)(x)
    x = Dropout(dropout_rate)(x)

    attention = AttentionLayer()(x)
    output = Dense(3, activation='linear')(attention)

    return Model(inputs=[sequence_input, entity_input], outputs=output)

model = build_entity_attention_lstm(
    input_seq_shape=(X_train_clean.shape[1], X_train_clean.shape[2]),
    n_entities=len(entity_encoder.classes_)
)

model.compile(optimizer='adam', loss='mae', metrics=['mae'])
model.summary()

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)

history = model.fit(
    [X_train_clean, entity_train_encoded],
    y_train_clean,
    validation_data=([X_val_clean, entity_val_encoded], y_val_clean),
    epochs=15,
    batch_size=32,
    callbacks=[early_stop],
    verbose=1
)

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Predict on val
y_val_pred_scaled = model.predict([X_val_clean, entity_val_encoded])
y_val_pred = scaler.inverse_transform(y_val_pred_scaled)
y_val_true = scaler.inverse_transform(y_val_clean)

# Evaluate
mae = mean_absolute_error(y_val_true, y_val_pred)
rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))
print(f"âœ… Inverse-Scaled MAE: {mae:.4f}")
print(f"âœ… Inverse-Scaled RMSE: {rmse:.4f}")

# Directional accuracy
direction_true = (y_val_true > 0).astype(int)
direction_pred = (y_val_pred > 0).astype(int)
directional_accuracy = (direction_true == direction_pred).mean()
print(f"âœ… Directional Accuracy: {directional_accuracy:.4f}")

"""#Merge Master_df and train,val,test df to get the 60 day windows prior to the volatile day"""

import numpy as np
import pandas as pd
from tqdm import tqdm

# Final selected features (streak_id removed)
final_features = [
    'daily_return', 'open', 'return_z_score', 'avg_volume_20d', 'market_cap',
    'net_income', 'credit_rating_rank', 'book_to_market', 'book_value_equity',
    'close', 'volume', 'was_not_rated', 'common_dividend_declared_2024',
    'sma_10', 'total_rev_2024', 'credit_rating_rank_missing',
    'avg_market_volatility', 'month', 'debt_to_equity_2024',
    'rsi_14', 'momentum_10d'
]

# Columns to exclude from the modeling input
exclude_cols = [
    'entity_name', 'ticker', 'date', 'prev_date', 'day_diff',
    'shock_polarity', 'is_volatility_shock', 'is_vol_shock_day',
    'new_streak',  # streak_id also now excluded
    'target_return_1d', 'target_return_5d', 'target_return_20d',
    'entity_ID_encoded'
]

# Ensure we only use the relevant columns from master_df
required_cols = ['entity_ID', 'date'] + final_features
master_filtered = master_df[required_cols].copy()
master_filtered['date'] = pd.to_datetime(master_filtered['date'])

# Function to retrieve a 60-day window before the shock date
def get_window(row, master_df, window_size=60):
    entity_id = row['entity_ID']
    shock_date = pd.to_datetime(row['date'])

    mask = (master_df['entity_ID'] == entity_id) & (master_df['date'] < shock_date)
    entity_data = master_df.loc[mask].sort_values('date', ascending=False).head(window_size)

    # Pad with NaNs if fewer than 60 days
    if len(entity_data) < window_size:
        padding = pd.DataFrame(np.nan, index=np.arange(window_size - len(entity_data)), columns=entity_data.columns)
        entity_data = pd.concat([padding, entity_data], ignore_index=True)

    return entity_data[final_features].values

# Merge windows into train, val, test
def attach_windows(df, master_df, name=''):
    print(f"Attaching feature windows for {name} set...")
    windows = []
    for _, row in tqdm(df.iterrows(), total=len(df)):
        win = get_window(row, master_df)
        windows.append(win)
    df['feature_window'] = windows
    return df

# Run the window attachment
train_df = attach_windows(train_df, master_filtered, name='Train')
val_df = attach_windows(val_df, master_filtered, name='Validation')
test_df = attach_windows(test_df, master_filtered, name='Test')

# Show final column names
print("Final columns in train_df:")
print(train_df.columns.tolist())

"""#LSTM Model Development







*   Prepare LSTM input format: (samples, time steps, features)
* Include:
  * Entity embeddings
  * Time sequences
  * Dropout / regularization
  * Train using sliding windows (if needed) to preserve sequence

We are building the final dataset with all values 60 days before the volatile event. If any day's values are missing (first 60 days of data) we will pad them with -999 and then add a masking to the LSTM model that ignores these values.

### ** Some of our targets have NaaN's which cannot be passed to the model, so we had to drop the targets containing NaN... Here's why: **

Justification for Dropping NaNs in Target Variables
Why the NaNs exist:

Each target_return_Xd is computed using future price movements following a volatility event.

If there arenâ€™t enough days ahead in the dataset (e.g. 5 or 20 trading days), you cannot compute that return, and so a NaN is expected.

Why itâ€™s okay to drop them:

These NaNs do not represent data quality issues â€” they are deterministic based on future availability.

They occur only near the dataset's end, so dropping them doesnâ€™t bias your training, as their absence isnâ€™t related to the stockâ€™s characteristics or model input.

You still retain 96%+ of your data for the most affected target.
"""

# Drop rows with missing target values from train/val/test
def drop_missing_targets(df, target_cols):
    initial_len = len(df)
    df_clean = df.dropna(subset=target_cols).reset_index(drop=True)
    final_len = len(df_clean)
    print(f"Dropped {initial_len - final_len} rows ({(initial_len - final_len)/initial_len:.2%}) due to missing targets")
    return df_clean

# Define target columns
target_cols = ['target_return_1d', 'target_return_5d', 'target_return_20d']

# Apply to each dataset
train_df = drop_missing_targets(train_df, target_cols)
val_df = drop_missing_targets(val_df, target_cols)
test_df = drop_missing_targets(test_df, target_cols)

# Confirm final sizes
print(f"\nâœ… Final sizes:")
print(f"Train: {len(train_df)}")
print(f"Validation: {len(val_df)}")
print(f"Test: {len(test_df)}")

import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Masking, Input
from tensorflow.keras.callbacks import EarlyStopping

# -----------------------------
# Prepare LSTM Input & Targets
# -----------------------------
def prepare_lstm_data(df):
    X = np.stack(df['feature_window'].values)
    y_1d = df['target_return_1d'].values
    y_5d = df['target_return_5d'].values
    y_20d = df['target_return_20d'].values
    return X, y_1d, y_5d, y_20d

# Prepare data from train and validation sets only
X_train, y1_train, y5_train, y20_train = prepare_lstm_data(train_df)
X_val, y1_val, y5_val, y20_val = prepare_lstm_data(val_df)

# -----------------------------
# Mask Invalid Sequences (NaNs)
# -----------------------------
X_train_masked = np.where(np.isnan(X_train), 0.0, X_train)
X_val_masked = np.where(np.isnan(X_val), 0.0, X_val)

# -----------------------------
# Build and Train LSTM
# -----------------------------
def build_lstm_model(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Masking(mask_value=0.0),
        LSTM(64, return_sequences=False),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

def train_and_validate(X_train, y_train, X_val, y_val, horizon_label='1-day'):
    print(f"\nğŸ” Training model for {horizon_label} return...\n")

    model = build_lstm_model(input_shape=X_train.shape[1:])
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=25,
        batch_size=32,
        callbacks=[early_stop],
        verbose=1
    )

    return model, history

# -----------------------------
# Train for Each Horizon
# -----------------------------
model_1d, hist_1d = train_and_validate(X_train_masked, y1_train, X_val_masked, y1_val, '1-day')
model_5d, hist_5d = train_and_validate(X_train_masked, y5_train, X_val_masked, y5_val, '5-day')
model_20d, hist_20d = train_and_validate(X_train_masked, y20_train, X_val_masked, y20_val, '20-day')

"""Evaluation of preliminary LSTM model against validation set"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np

def evaluate_predictions(model, X_val, y_val, horizon_label=''):
    y_pred = model.predict(X_val).flatten()

    mse = mean_squared_error(y_val, y_pred)
    mae = mean_absolute_error(y_val, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_val, y_pred)

    print(f"\nğŸ“Š Validation Metrics for {horizon_label}:")
    print(f"  - MAE : {mae:.5f}")
    print(f"  - RMSE: {rmse:.5f}")
    print(f"  - RÂ²  : {r2:.5f}")

    return y_pred

# Evaluate on validation set
preds_1d_val = evaluate_predictions(model_1d, X_val_masked, y1_val, '1-Day')
preds_5d_val = evaluate_predictions(model_5d, X_val_masked, y5_val, '5-Day')
preds_20d_val = evaluate_predictions(model_20d, X_val_masked, y20_val, '20-Day')

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import r2_score

def plot_actual_vs_predicted(y_true, y_pred, horizon_label):
    plt.figure(figsize=(6, 6))
    sns.scatterplot(x=y_true, y=y_pred, alpha=0.3)
    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--', color='red', label='Perfect prediction')
    plt.title(f'{horizon_label} Return: Actual vs. Predicted')
    plt.xlabel('Actual Return')
    plt.ylabel('Predicted Return')
    plt.legend()
    plt.grid(True)
    plt.show()

    r2 = r2_score(y_true, y_pred)
    print(f"RÂ² for {horizon_label}: {r2:.5f}")

# Generate predictions on validation set
y1_val_pred = model_1d.predict(X_val_masked).flatten()
y5_val_pred = model_5d.predict(X_val_masked).flatten()
y20_val_pred = model_20d.predict(X_val_masked).flatten()

# Plot
plot_actual_vs_predicted(y1_val, y1_val_pred, '1-Day')
plot_actual_vs_predicted(y5_val, y5_val_pred, '5-Day')
plot_actual_vs_predicted(y20_val, y20_val_pred, '20-Day')

"""###Hyperparameter Tuning



Applies to all models: Grid search or random search on:
* Learning rate

* Window size

* Hidden layers

* Dropout rate

* Optimizer choice


"""

